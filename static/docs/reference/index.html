<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Function reference â€¢ torch</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet"><meta property="og:title" content="Function reference"><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-9ZJSKW3L0N"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9ZJSKW3L0N');
</script></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-dark navbar-expand-lg bg-primary" data-bs-theme="dark"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">torch</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.13.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/installation.html">Installation</a>
    <h6 class="dropdown-header" data-toc-skip>Tensors</h6>
    <a class="dropdown-item" href="../articles/tensor-creation.html">Creating tensors</a>
    <a class="dropdown-item" href="../articles/indexing.html">Indexing</a>
    <a class="dropdown-item" href="../articles/tensor/index.html">Tensor class</a>
    <a class="dropdown-item" href="../articles/serialization.html">Serialization</a>
    <h6 class="dropdown-header" data-toc-skip>Datasets</h6>
    <a class="dropdown-item" href="../articles/loading-data.html">Loading Data</a>
    <h6 class="dropdown-header" data-toc-skip>Autograd</h6>
    <a class="dropdown-item" href="../articles/using-autograd.html">Using autograd</a>
    <a class="dropdown-item" href="../articles/extending-autograd.html">Extending autograd</a>
    <a class="dropdown-item" href="../articles/python-to-r.html">Python models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-examples">Examples</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-examples">
    <a class="dropdown-item" href="../articles/examples/basic-autograd.html">basic-autograd</a>
    <a class="dropdown-item" href="../articles/examples/basic-nn-module.html">basic-nn-module</a>
    <a class="dropdown-item" href="../articles/examples/dataset.html">dataset</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-advanced">Advanced</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-advanced">
    <a class="dropdown-item" href="../articles/memory-management.html">Memory management</a>
    <a class="dropdown-item" href="../articles/modifying-source-code.html">Building locally</a>
    <a class="dropdown-item" href="../articles/amp.html">Automatic Mixed Precision</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"><li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/mlverse/torch/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div>

    
  </div>
</nav><div class="container template-reference-index">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Function reference</h1>
    </div>

    <div class="section level2">
      <h2 id="tensor-creation-utilities">Tensor creation utilities<a class="anchor" aria-label="anchor" href="#tensor-creation-utilities"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="torch_empty.html">torch_empty()</a></code> 
        </dt>
        <dd>Empty</dd>
      </dl><dl><dt>
          
          <code><a href="torch_arange.html">torch_arange()</a></code> 
        </dt>
        <dd>Arange</dd>
      </dl><dl><dt>
          
          <code><a href="torch_eye.html">torch_eye()</a></code> 
        </dt>
        <dd>Eye</dd>
      </dl><dl><dt>
          
          <code><a href="torch_full.html">torch_full()</a></code> 
        </dt>
        <dd>Full</dd>
      </dl><dl><dt>
          
          <code><a href="torch_linspace.html">torch_linspace()</a></code> 
        </dt>
        <dd>Linspace</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logspace.html">torch_logspace()</a></code> 
        </dt>
        <dd>Logspace</dd>
      </dl><dl><dt>
          
          <code><a href="torch_ones.html">torch_ones()</a></code> 
        </dt>
        <dd>Ones</dd>
      </dl><dl><dt>
          
          <code><a href="torch_rand.html">torch_rand()</a></code> 
        </dt>
        <dd>Rand</dd>
      </dl><dl><dt>
          
          <code><a href="torch_randint.html">torch_randint()</a></code> 
        </dt>
        <dd>Randint</dd>
      </dl><dl><dt>
          
          <code><a href="torch_randn.html">torch_randn()</a></code> 
        </dt>
        <dd>Randn</dd>
      </dl><dl><dt>
          
          <code><a href="torch_randperm.html">torch_randperm()</a></code> 
        </dt>
        <dd>Randperm</dd>
      </dl><dl><dt>
          
          <code><a href="torch_zeros.html">torch_zeros()</a></code> 
        </dt>
        <dd>Zeros</dd>
      </dl><dl><dt>
          
          <code><a href="torch_empty_like.html">torch_empty_like()</a></code> 
        </dt>
        <dd>Empty_like</dd>
      </dl><dl><dt>
          
          <code><a href="torch_full_like.html">torch_full_like()</a></code> 
        </dt>
        <dd>Full_like</dd>
      </dl><dl><dt>
          
          <code><a href="torch_ones_like.html">torch_ones_like()</a></code> 
        </dt>
        <dd>Ones_like</dd>
      </dl><dl><dt>
          
          <code><a href="torch_rand_like.html">torch_rand_like()</a></code> 
        </dt>
        <dd>Rand_like</dd>
      </dl><dl><dt>
          
          <code><a href="torch_randint_like.html">torch_randint_like()</a></code> 
        </dt>
        <dd>Randint_like</dd>
      </dl><dl><dt>
          
          <code><a href="torch_randn_like.html">torch_randn_like()</a></code> 
        </dt>
        <dd>Randn_like</dd>
      </dl><dl><dt>
          
          <code><a href="torch_zeros_like.html">torch_zeros_like()</a></code> 
        </dt>
        <dd>Zeros_like</dd>
      </dl><dl><dt>
          
          <code><a href="as_array.html">as_array()</a></code> 
        </dt>
        <dd>Converts to array</dd>
      </dl><dl><dt>
          
          <code><a href="torch_tensor_from_buffer.html">torch_tensor_from_buffer()</a></code> <code><a href="torch_tensor_from_buffer.html">buffer_from_torch_tensor()</a></code> 
        </dt>
        <dd>Creates a tensor from a buffer of memory</dd>
      </dl></div><div class="section level2">
      <h2 id="tensor-attributes">Tensor attributes<a class="anchor" aria-label="anchor" href="#tensor-attributes"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="default_dtype.html">torch_set_default_dtype()</a></code> <code><a href="default_dtype.html">torch_get_default_dtype()</a></code> 
        </dt>
        <dd>Gets and sets the default floating point dtype.</dd>
      </dl><dl><dt>
          
          <code><a href="is_torch_device.html">is_torch_device()</a></code> 
        </dt>
        <dd>Checks if object is a device</dd>
      </dl><dl><dt>
          
          <code><a href="is_torch_dtype.html">is_torch_dtype()</a></code> 
        </dt>
        <dd>Check if object is a torch data type</dd>
      </dl><dl><dt>
          
          <code><a href="torch_dtype.html">torch_float32()</a></code> <code><a href="torch_dtype.html">torch_float()</a></code> <code><a href="torch_dtype.html">torch_float64()</a></code> <code><a href="torch_dtype.html">torch_double()</a></code> <code><a href="torch_dtype.html">torch_cfloat32()</a></code> <code><a href="torch_dtype.html">torch_chalf()</a></code> <code><a href="torch_dtype.html">torch_cfloat()</a></code> <code><a href="torch_dtype.html">torch_cfloat64()</a></code> <code><a href="torch_dtype.html">torch_cdouble()</a></code> <code><a href="torch_dtype.html">torch_cfloat128()</a></code> <code><a href="torch_dtype.html">torch_float16()</a></code> <code><a href="torch_dtype.html">torch_half()</a></code> <code><a href="torch_dtype.html">torch_uint8()</a></code> <code><a href="torch_dtype.html">torch_int8()</a></code> <code><a href="torch_dtype.html">torch_int16()</a></code> <code><a href="torch_dtype.html">torch_short()</a></code> <code><a href="torch_dtype.html">torch_int32()</a></code> <code><a href="torch_dtype.html">torch_int()</a></code> <code><a href="torch_dtype.html">torch_int64()</a></code> <code><a href="torch_dtype.html">torch_long()</a></code> <code><a href="torch_dtype.html">torch_bool()</a></code> <code><a href="torch_dtype.html">torch_quint8()</a></code> <code><a href="torch_dtype.html">torch_qint8()</a></code> <code><a href="torch_dtype.html">torch_qint32()</a></code> 
        </dt>
        <dd>Torch data types</dd>
      </dl><dl><dt>
          
          <code><a href="torch_finfo.html">torch_finfo()</a></code> 
        </dt>
        <dd>Floating point type info</dd>
      </dl><dl><dt>
          
          <code><a href="torch_iinfo.html">torch_iinfo()</a></code> 
        </dt>
        <dd>Integer type info</dd>
      </dl><dl><dt>
          
          <code><a href="torch_qscheme.html">torch_per_channel_affine()</a></code> <code><a href="torch_qscheme.html">torch_per_tensor_affine()</a></code> <code><a href="torch_qscheme.html">torch_per_channel_symmetric()</a></code> <code><a href="torch_qscheme.html">torch_per_tensor_symmetric()</a></code> 
        </dt>
        <dd>Creates the corresponding Scheme object</dd>
      </dl><dl><dt>
          
          <code><a href="torch_reduction.html">torch_reduction_sum()</a></code> <code><a href="torch_reduction.html">torch_reduction_mean()</a></code> <code><a href="torch_reduction.html">torch_reduction_none()</a></code> 
        </dt>
        <dd>Creates the reduction objet</dd>
      </dl><dl><dt>
          
          <code><a href="is_torch_layout.html">is_torch_layout()</a></code> 
        </dt>
        <dd>Check if an object is a torch layout.</dd>
      </dl><dl><dt>
          
          <code><a href="is_torch_memory_format.html">is_torch_memory_format()</a></code> 
        </dt>
        <dd>Check if an object is a memory format</dd>
      </dl><dl><dt>
          
          <code><a href="is_torch_qscheme.html">is_torch_qscheme()</a></code> 
        </dt>
        <dd>Checks if an object is a QScheme</dd>
      </dl><dl><dt>
          
          <code><a href="is_undefined_tensor.html">is_undefined_tensor()</a></code> 
        </dt>
        <dd>Checks if a tensor is undefined</dd>
      </dl></div><div class="section level2">
      <h2 id="serialization">Serialization<a class="anchor" aria-label="anchor" href="#serialization"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="load_state_dict.html">load_state_dict()</a></code> 
        </dt>
        <dd>Load a state dict file</dd>
      </dl><dl><dt>
          
          <code><a href="torch_load.html">torch_load()</a></code> 
        </dt>
        <dd>Loads a saved object</dd>
      </dl><dl><dt>
          
          <code><a href="torch_save.html">torch_save()</a></code> 
        </dt>
        <dd>Saves an object to a disk file.</dd>
      </dl><dl><dt>
          
          <code><a href="torch_serialize.html">torch_serialize()</a></code> 
        </dt>
        <dd>Serialize a torch object returning a raw object</dd>
      </dl><dl><dt>
          
          <code><a href="clone_module.html">clone_module()</a></code> 
        </dt>
        <dd>Clone a torch module.</dd>
      </dl></div><div class="section level2">
      <h2 id="mathematical-operations-on-tensors">Mathematical operations on tensors<a class="anchor" aria-label="anchor" href="#mathematical-operations-on-tensors"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="threads.html">torch_set_num_threads()</a></code> <code><a href="threads.html">torch_set_num_interop_threads()</a></code> <code><a href="threads.html">torch_get_num_interop_threads()</a></code> <code><a href="threads.html">torch_get_num_threads()</a></code> 
        </dt>
        <dd>Number of threads</dd>
      </dl><dl><dt>
          
          <code><a href="torch_abs.html">torch_abs()</a></code> 
        </dt>
        <dd>Abs</dd>
      </dl><dl><dt>
          
          <code><a href="torch_absolute.html">torch_absolute()</a></code> 
        </dt>
        <dd>Absolute</dd>
      </dl><dl><dt>
          
          <code><a href="torch_acos.html">torch_acos()</a></code> 
        </dt>
        <dd>Acos</dd>
      </dl><dl><dt>
          
          <code><a href="torch_acosh.html">torch_acosh()</a></code> 
        </dt>
        <dd>Acosh</dd>
      </dl><dl><dt>
          
          <code><a href="torch_adaptive_avg_pool1d.html">torch_adaptive_avg_pool1d()</a></code> 
        </dt>
        <dd>Adaptive_avg_pool1d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_add.html">torch_add()</a></code> 
        </dt>
        <dd>Add</dd>
      </dl><dl><dt>
          
          <code><a href="torch_addbmm.html">torch_addbmm()</a></code> 
        </dt>
        <dd>Addbmm</dd>
      </dl><dl><dt>
          
          <code><a href="torch_addcdiv.html">torch_addcdiv()</a></code> 
        </dt>
        <dd>Addcdiv</dd>
      </dl><dl><dt>
          
          <code><a href="torch_addcmul.html">torch_addcmul()</a></code> 
        </dt>
        <dd>Addcmul</dd>
      </dl><dl><dt>
          
          <code><a href="torch_addmm.html">torch_addmm()</a></code> 
        </dt>
        <dd>Addmm</dd>
      </dl><dl><dt>
          
          <code><a href="torch_addmv.html">torch_addmv()</a></code> 
        </dt>
        <dd>Addmv</dd>
      </dl><dl><dt>
          
          <code><a href="torch_addr.html">torch_addr()</a></code> 
        </dt>
        <dd>Addr</dd>
      </dl><dl><dt>
          
          <code><a href="torch_allclose.html">torch_allclose()</a></code> 
        </dt>
        <dd>Allclose</dd>
      </dl><dl><dt>
          
          <code><a href="torch_amax.html">torch_amax()</a></code> 
        </dt>
        <dd>Amax</dd>
      </dl><dl><dt>
          
          <code><a href="torch_amin.html">torch_amin()</a></code> 
        </dt>
        <dd>Amin</dd>
      </dl><dl><dt>
          
          <code><a href="torch_angle.html">torch_angle()</a></code> 
        </dt>
        <dd>Angle</dd>
      </dl><dl><dt>
          
          <code><a href="torch_arccos.html">torch_arccos()</a></code> 
        </dt>
        <dd>Arccos</dd>
      </dl><dl><dt>
          
          <code><a href="torch_arccosh.html">torch_arccosh()</a></code> 
        </dt>
        <dd>Arccosh</dd>
      </dl><dl><dt>
          
          <code><a href="torch_arcsin.html">torch_arcsin()</a></code> 
        </dt>
        <dd>Arcsin</dd>
      </dl><dl><dt>
          
          <code><a href="torch_arcsinh.html">torch_arcsinh()</a></code> 
        </dt>
        <dd>Arcsinh</dd>
      </dl><dl><dt>
          
          <code><a href="torch_arctan.html">torch_arctan()</a></code> 
        </dt>
        <dd>Arctan</dd>
      </dl><dl><dt>
          
          <code><a href="torch_arctanh.html">torch_arctanh()</a></code> 
        </dt>
        <dd>Arctanh</dd>
      </dl><dl><dt>
          
          <code><a href="torch_argmax.html">torch_argmax</a></code> 
        </dt>
        <dd>Argmax</dd>
      </dl><dl><dt>
          
          <code><a href="torch_argmin.html">torch_argmin</a></code> 
        </dt>
        <dd>Argmin</dd>
      </dl><dl><dt>
          
          <code><a href="torch_argsort.html">torch_argsort()</a></code> 
        </dt>
        <dd>Argsort</dd>
      </dl><dl><dt>
          
          <code><a href="torch_as_strided.html">torch_as_strided()</a></code> 
        </dt>
        <dd>As_strided</dd>
      </dl><dl><dt>
          
          <code><a href="torch_asin.html">torch_asin()</a></code> 
        </dt>
        <dd>Asin</dd>
      </dl><dl><dt>
          
          <code><a href="torch_asinh.html">torch_asinh()</a></code> 
        </dt>
        <dd>Asinh</dd>
      </dl><dl><dt>
          
          <code><a href="torch_atan.html">torch_atan()</a></code> 
        </dt>
        <dd>Atan</dd>
      </dl><dl><dt>
          
          <code><a href="torch_atan2.html">torch_atan2()</a></code> 
        </dt>
        <dd>Atan2</dd>
      </dl><dl><dt>
          
          <code><a href="torch_atanh.html">torch_atanh()</a></code> 
        </dt>
        <dd>Atanh</dd>
      </dl><dl><dt>
          
          <code><a href="torch_atleast_1d.html">torch_atleast_1d()</a></code> 
        </dt>
        <dd>Atleast_1d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_atleast_2d.html">torch_atleast_2d()</a></code> 
        </dt>
        <dd>Atleast_2d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_atleast_3d.html">torch_atleast_3d()</a></code> 
        </dt>
        <dd>Atleast_3d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_avg_pool1d.html">torch_avg_pool1d()</a></code> 
        </dt>
        <dd>Avg_pool1d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_baddbmm.html">torch_baddbmm()</a></code> 
        </dt>
        <dd>Baddbmm</dd>
      </dl><dl><dt>
          
          <code><a href="torch_bartlett_window.html">torch_bartlett_window()</a></code> 
        </dt>
        <dd>Bartlett_window</dd>
      </dl><dl><dt>
          
          <code><a href="torch_bernoulli.html">torch_bernoulli()</a></code> 
        </dt>
        <dd>Bernoulli</dd>
      </dl><dl><dt>
          
          <code><a href="torch_bincount.html">torch_bincount</a></code> 
        </dt>
        <dd>Bincount</dd>
      </dl><dl><dt>
          
          <code><a href="torch_bitwise_and.html">torch_bitwise_and()</a></code> 
        </dt>
        <dd>Bitwise_and</dd>
      </dl><dl><dt>
          
          <code><a href="torch_bitwise_not.html">torch_bitwise_not()</a></code> 
        </dt>
        <dd>Bitwise_not</dd>
      </dl><dl><dt>
          
          <code><a href="torch_bitwise_or.html">torch_bitwise_or()</a></code> 
        </dt>
        <dd>Bitwise_or</dd>
      </dl><dl><dt>
          
          <code><a href="torch_bitwise_xor.html">torch_bitwise_xor()</a></code> 
        </dt>
        <dd>Bitwise_xor</dd>
      </dl><dl><dt>
          
          <code><a href="torch_blackman_window.html">torch_blackman_window()</a></code> 
        </dt>
        <dd>Blackman_window</dd>
      </dl><dl><dt>
          
          <code><a href="torch_block_diag.html">torch_block_diag()</a></code> 
        </dt>
        <dd>Block_diag</dd>
      </dl><dl><dt>
          
          <code><a href="torch_bmm.html">torch_bmm()</a></code> 
        </dt>
        <dd>Bmm</dd>
      </dl><dl><dt>
          
          <code><a href="torch_broadcast_tensors.html">torch_broadcast_tensors()</a></code> 
        </dt>
        <dd>Broadcast_tensors</dd>
      </dl><dl><dt>
          
          <code><a href="torch_bucketize.html">torch_bucketize()</a></code> 
        </dt>
        <dd>Bucketize</dd>
      </dl><dl><dt>
          
          <code><a href="torch_can_cast.html">torch_can_cast()</a></code> 
        </dt>
        <dd>Can_cast</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cartesian_prod.html">torch_cartesian_prod()</a></code> 
        </dt>
        <dd>Cartesian_prod</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cat.html">torch_cat()</a></code> 
        </dt>
        <dd>Cat</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cdist.html">torch_cdist()</a></code> 
        </dt>
        <dd>Cdist</dd>
      </dl><dl><dt>
          
          <code><a href="torch_ceil.html">torch_ceil()</a></code> 
        </dt>
        <dd>Ceil</dd>
      </dl><dl><dt>
          
          <code><a href="torch_celu.html">torch_celu()</a></code> 
        </dt>
        <dd>Celu</dd>
      </dl><dl><dt>
          
          <code><a href="torch_celu_.html">torch_celu_()</a></code> 
        </dt>
        <dd>Celu_</dd>
      </dl><dl><dt>
          
          <code><a href="torch_chain_matmul.html">torch_chain_matmul()</a></code> 
        </dt>
        <dd>Chain_matmul</dd>
      </dl><dl><dt>
          
          <code><a href="torch_channel_shuffle.html">torch_channel_shuffle()</a></code> 
        </dt>
        <dd>Channel_shuffle</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cholesky.html">torch_cholesky()</a></code> 
        </dt>
        <dd>Cholesky</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cholesky_inverse.html">torch_cholesky_inverse()</a></code> 
        </dt>
        <dd>Cholesky_inverse</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cholesky_solve.html">torch_cholesky_solve()</a></code> 
        </dt>
        <dd>Cholesky_solve</dd>
      </dl><dl><dt>
          
          <code><a href="torch_chunk.html">torch_chunk()</a></code> 
        </dt>
        <dd>Chunk</dd>
      </dl><dl><dt>
          
          <code><a href="torch_clamp.html">torch_clamp()</a></code> 
        </dt>
        <dd>Clamp</dd>
      </dl><dl><dt>
          
          <code><a href="torch_clip.html">torch_clip()</a></code> 
        </dt>
        <dd>Clip</dd>
      </dl><dl><dt>
          
          <code><a href="torch_clone.html">torch_clone()</a></code> 
        </dt>
        <dd>Clone</dd>
      </dl><dl><dt>
          
          <code><a href="torch_combinations.html">torch_combinations()</a></code> 
        </dt>
        <dd>Combinations</dd>
      </dl><dl><dt>
          
          <code><a href="torch_complex.html">torch_complex()</a></code> 
        </dt>
        <dd>Complex</dd>
      </dl><dl><dt>
          
          <code><a href="torch_conj.html">torch_conj()</a></code> 
        </dt>
        <dd>Conj</dd>
      </dl><dl><dt>
          
          <code><a href="torch_conv1d.html">torch_conv1d()</a></code> 
        </dt>
        <dd>Conv1d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_conv2d.html">torch_conv2d()</a></code> 
        </dt>
        <dd>Conv2d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_conv3d.html">torch_conv3d()</a></code> 
        </dt>
        <dd>Conv3d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_conv_tbc.html">torch_conv_tbc()</a></code> 
        </dt>
        <dd>Conv_tbc</dd>
      </dl><dl><dt>
          
          <code><a href="torch_conv_transpose1d.html">torch_conv_transpose1d()</a></code> 
        </dt>
        <dd>Conv_transpose1d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_conv_transpose2d.html">torch_conv_transpose2d()</a></code> 
        </dt>
        <dd>Conv_transpose2d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_conv_transpose3d.html">torch_conv_transpose3d()</a></code> 
        </dt>
        <dd>Conv_transpose3d</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cos.html">torch_cos()</a></code> 
        </dt>
        <dd>Cos</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cosh.html">torch_cosh()</a></code> 
        </dt>
        <dd>Cosh</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cosine_similarity.html">torch_cosine_similarity()</a></code> 
        </dt>
        <dd>Cosine_similarity</dd>
      </dl><dl><dt>
          
          <code><a href="torch_count_nonzero.html">torch_count_nonzero()</a></code> 
        </dt>
        <dd>Count_nonzero</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cross.html">torch_cross()</a></code> 
        </dt>
        <dd>Cross</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cummax.html">torch_cummax()</a></code> 
        </dt>
        <dd>Cummax</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cummin.html">torch_cummin()</a></code> 
        </dt>
        <dd>Cummin</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cumprod.html">torch_cumprod()</a></code> 
        </dt>
        <dd>Cumprod</dd>
      </dl><dl><dt>
          
          <code><a href="torch_cumsum.html">torch_cumsum()</a></code> 
        </dt>
        <dd>Cumsum</dd>
      </dl><dl><dt>
          
          <code><a href="torch_deg2rad.html">torch_deg2rad()</a></code> 
        </dt>
        <dd>Deg2rad</dd>
      </dl><dl><dt>
          
          <code><a href="torch_dequantize.html">torch_dequantize()</a></code> 
        </dt>
        <dd>Dequantize</dd>
      </dl><dl><dt>
          
          <code><a href="torch_det.html">torch_det()</a></code> 
        </dt>
        <dd>Det</dd>
      </dl><dl><dt>
          
          <code><a href="torch_device.html">torch_device()</a></code> 
        </dt>
        <dd>Create a Device object</dd>
      </dl><dl><dt>
          
          <code><a href="torch_diag.html">torch_diag()</a></code> 
        </dt>
        <dd>Diag</dd>
      </dl><dl><dt>
          
          <code><a href="torch_diag_embed.html">torch_diag_embed()</a></code> 
        </dt>
        <dd>Diag_embed</dd>
      </dl><dl><dt>
          
          <code><a href="torch_diagflat.html">torch_diagflat()</a></code> 
        </dt>
        <dd>Diagflat</dd>
      </dl><dl><dt>
          
          <code><a href="torch_diagonal.html">torch_diagonal()</a></code> 
        </dt>
        <dd>Diagonal</dd>
      </dl><dl><dt>
          
          <code><a href="torch_diff.html">torch_diff()</a></code> 
        </dt>
        <dd>Computes the n-th forward difference along the given dimension.</dd>
      </dl><dl><dt>
          
          <code><a href="torch_digamma.html">torch_digamma()</a></code> 
        </dt>
        <dd>Digamma</dd>
      </dl><dl><dt>
          
          <code><a href="torch_dist.html">torch_dist()</a></code> 
        </dt>
        <dd>Dist</dd>
      </dl><dl><dt>
          
          <code><a href="torch_div.html">torch_div()</a></code> 
        </dt>
        <dd>Div</dd>
      </dl><dl><dt>
          
          <code><a href="torch_divide.html">torch_divide()</a></code> 
        </dt>
        <dd>Divide</dd>
      </dl><dl><dt>
          
          <code><a href="torch_dot.html">torch_dot()</a></code> 
        </dt>
        <dd>Dot</dd>
      </dl><dl><dt>
          
          <code><a href="torch_dstack.html">torch_dstack()</a></code> 
        </dt>
        <dd>Dstack</dd>
      </dl><dl><dt>
          
          <code><a href="torch_eig.html">torch_eig</a></code> 
        </dt>
        <dd>Eig</dd>
      </dl><dl><dt>
          
          <code><a href="torch_einsum.html">torch_einsum()</a></code> 
        </dt>
        <dd>Einsum</dd>
      </dl><dl><dt>
          
          <code><a href="torch_empty_strided.html">torch_empty_strided()</a></code> 
        </dt>
        <dd>Empty_strided</dd>
      </dl><dl><dt>
          
          <code><a href="torch_eq.html">torch_eq()</a></code> 
        </dt>
        <dd>Eq</dd>
      </dl><dl><dt>
          
          <code><a href="torch_equal.html">torch_equal()</a></code> 
        </dt>
        <dd>Equal</dd>
      </dl><dl><dt>
          
          <code><a href="torch_erf.html">torch_erf()</a></code> 
        </dt>
        <dd>Erf</dd>
      </dl><dl><dt>
          
          <code><a href="torch_erfc.html">torch_erfc()</a></code> 
        </dt>
        <dd>Erfc</dd>
      </dl><dl><dt>
          
          <code><a href="torch_erfinv.html">torch_erfinv()</a></code> 
        </dt>
        <dd>Erfinv</dd>
      </dl><dl><dt>
          
          <code><a href="torch_exp.html">torch_exp()</a></code> 
        </dt>
        <dd>Exp</dd>
      </dl><dl><dt>
          
          <code><a href="torch_exp2.html">torch_exp2()</a></code> 
        </dt>
        <dd>Exp2</dd>
      </dl><dl><dt>
          
          <code><a href="torch_expm1.html">torch_expm1()</a></code> 
        </dt>
        <dd>Expm1</dd>
      </dl><dl><dt>
          
          <code><a href="torch_fft_fft.html">torch_fft_fft()</a></code> 
        </dt>
        <dd>Fft</dd>
      </dl><dl><dt>
          
          <code><a href="torch_fft_fftfreq.html">torch_fft_fftfreq()</a></code> 
        </dt>
        <dd>fftfreq</dd>
      </dl><dl><dt>
          
          <code><a href="torch_fft_ifft.html">torch_fft_ifft()</a></code> 
        </dt>
        <dd>Ifft</dd>
      </dl><dl><dt>
          
          <code><a href="torch_fft_irfft.html">torch_fft_irfft()</a></code> 
        </dt>
        <dd>Irfft</dd>
      </dl><dl><dt>
          
          <code><a href="torch_fft_rfft.html">torch_fft_rfft()</a></code> 
        </dt>
        <dd>Rfft</dd>
      </dl><dl><dt>
          
          <code><a href="torch_fix.html">torch_fix()</a></code> 
        </dt>
        <dd>Fix</dd>
      </dl><dl><dt>
          
          <code><a href="torch_flatten.html">torch_flatten()</a></code> 
        </dt>
        <dd>Flatten</dd>
      </dl><dl><dt>
          
          <code><a href="torch_flip.html">torch_flip()</a></code> 
        </dt>
        <dd>Flip</dd>
      </dl><dl><dt>
          
          <code><a href="torch_fliplr.html">torch_fliplr()</a></code> 
        </dt>
        <dd>Fliplr</dd>
      </dl><dl><dt>
          
          <code><a href="torch_flipud.html">torch_flipud()</a></code> 
        </dt>
        <dd>Flipud</dd>
      </dl><dl><dt>
          
          <code><a href="torch_floor.html">torch_floor()</a></code> 
        </dt>
        <dd>Floor</dd>
      </dl><dl><dt>
          
          <code><a href="torch_floor_divide.html">torch_floor_divide()</a></code> 
        </dt>
        <dd>Floor_divide</dd>
      </dl><dl><dt>
          
          <code><a href="torch_fmod.html">torch_fmod()</a></code> 
        </dt>
        <dd>Fmod</dd>
      </dl><dl><dt>
          
          <code><a href="torch_frac.html">torch_frac()</a></code> 
        </dt>
        <dd>Frac</dd>
      </dl><dl><dt>
          
          <code><a href="torch_gather.html">torch_gather()</a></code> 
        </dt>
        <dd>Gather</dd>
      </dl><dl><dt>
          
          <code><a href="torch_gcd.html">torch_gcd()</a></code> 
        </dt>
        <dd>Gcd</dd>
      </dl><dl><dt>
          
          <code><a href="torch_ge.html">torch_ge()</a></code> 
        </dt>
        <dd>Ge</dd>
      </dl><dl><dt>
          
          <code><a href="torch_generator.html">torch_generator()</a></code> 
        </dt>
        <dd>Create a Generator object</dd>
      </dl><dl><dt>
          
          <code><a href="torch_geqrf.html">torch_geqrf()</a></code> 
        </dt>
        <dd>Geqrf</dd>
      </dl><dl><dt>
          
          <code><a href="torch_ger.html">torch_ger()</a></code> 
        </dt>
        <dd>Ger</dd>
      </dl><dl><dt>
          
          <code><a href="torch_get_rng_state.html">torch_get_rng_state()</a></code> <code><a href="torch_get_rng_state.html">torch_set_rng_state()</a></code> <code><a href="torch_get_rng_state.html">cuda_get_rng_state()</a></code> <code><a href="torch_get_rng_state.html">cuda_set_rng_state()</a></code> 
        </dt>
        <dd>RNG state management</dd>
      </dl><dl><dt>
          
          <code><a href="torch_greater.html">torch_greater()</a></code> 
        </dt>
        <dd>Greater</dd>
      </dl><dl><dt>
          
          <code><a href="torch_greater_equal.html">torch_greater_equal()</a></code> 
        </dt>
        <dd>Greater_equal</dd>
      </dl><dl><dt>
          
          <code><a href="torch_gt.html">torch_gt()</a></code> 
        </dt>
        <dd>Gt</dd>
      </dl><dl><dt>
          
          <code><a href="torch_hamming_window.html">torch_hamming_window()</a></code> 
        </dt>
        <dd>Hamming_window</dd>
      </dl><dl><dt>
          
          <code><a href="torch_hann_window.html">torch_hann_window()</a></code> 
        </dt>
        <dd>Hann_window</dd>
      </dl><dl><dt>
          
          <code><a href="torch_heaviside.html">torch_heaviside()</a></code> 
        </dt>
        <dd>Heaviside</dd>
      </dl><dl><dt>
          
          <code><a href="torch_histc.html">torch_histc()</a></code> 
        </dt>
        <dd>Histc</dd>
      </dl><dl><dt>
          
          <code><a href="torch_hstack.html">torch_hstack()</a></code> 
        </dt>
        <dd>Hstack</dd>
      </dl><dl><dt>
          
          <code><a href="torch_hypot.html">torch_hypot()</a></code> 
        </dt>
        <dd>Hypot</dd>
      </dl><dl><dt>
          
          <code><a href="torch_i0.html">torch_i0()</a></code> 
        </dt>
        <dd>I0</dd>
      </dl><dl><dt>
          
          <code><a href="torch_imag.html">torch_imag()</a></code> 
        </dt>
        <dd>Imag</dd>
      </dl><dl><dt>
          
          <code><a href="torch_index.html">torch_index()</a></code> 
        </dt>
        <dd>Index torch tensors</dd>
      </dl><dl><dt>
          
          <code><a href="torch_index_put.html">torch_index_put()</a></code> 
        </dt>
        <dd>Modify values selected by <code>indices</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="torch_index_put_.html">torch_index_put_()</a></code> 
        </dt>
        <dd>In-place version of <code>torch_index_put</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="torch_index_select.html">torch_index_select()</a></code> 
        </dt>
        <dd>Index_select</dd>
      </dl><dl><dt>
          
          <code><a href="torch_install_path.html">torch_install_path()</a></code> 
        </dt>
        <dd>A simple exported version of install_path Returns the torch installation path.</dd>
      </dl><dl><dt>
          
          <code><a href="torch_inverse.html">torch_inverse()</a></code> 
        </dt>
        <dd>Inverse</dd>
      </dl><dl><dt>
          
          <code><a href="torch_is_complex.html">torch_is_complex()</a></code> 
        </dt>
        <dd>Is_complex</dd>
      </dl><dl><dt>
          
          <code><a href="torch_is_floating_point.html">torch_is_floating_point()</a></code> 
        </dt>
        <dd>Is_floating_point</dd>
      </dl><dl><dt>
          
          <code><a href="torch_is_installed.html">torch_is_installed()</a></code> 
        </dt>
        <dd>Verifies if torch is installed</dd>
      </dl><dl><dt>
          
          <code><a href="torch_is_nonzero.html">torch_is_nonzero()</a></code> 
        </dt>
        <dd>Is_nonzero</dd>
      </dl><dl><dt>
          
          <code><a href="torch_isclose.html">torch_isclose()</a></code> 
        </dt>
        <dd>Isclose</dd>
      </dl><dl><dt>
          
          <code><a href="torch_isfinite.html">torch_isfinite()</a></code> 
        </dt>
        <dd>Isfinite</dd>
      </dl><dl><dt>
          
          <code><a href="torch_isinf.html">torch_isinf()</a></code> 
        </dt>
        <dd>Isinf</dd>
      </dl><dl><dt>
          
          <code><a href="torch_isnan.html">torch_isnan()</a></code> 
        </dt>
        <dd>Isnan</dd>
      </dl><dl><dt>
          
          <code><a href="torch_isneginf.html">torch_isneginf()</a></code> 
        </dt>
        <dd>Isneginf</dd>
      </dl><dl><dt>
          
          <code><a href="torch_isposinf.html">torch_isposinf()</a></code> 
        </dt>
        <dd>Isposinf</dd>
      </dl><dl><dt>
          
          <code><a href="torch_isreal.html">torch_isreal()</a></code> 
        </dt>
        <dd>Isreal</dd>
      </dl><dl><dt>
          
          <code><a href="torch_istft.html">torch_istft()</a></code> 
        </dt>
        <dd>Istft</dd>
      </dl><dl><dt>
          
          <code><a href="torch_kaiser_window.html">torch_kaiser_window()</a></code> 
        </dt>
        <dd>Kaiser_window</dd>
      </dl><dl><dt>
          
          <code><a href="torch_kron.html">torch_kron()</a></code> 
        </dt>
        <dd>Kronecker product</dd>
      </dl><dl><dt>
          
          <code><a href="torch_kthvalue.html">torch_kthvalue()</a></code> 
        </dt>
        <dd>Kthvalue</dd>
      </dl><dl><dt>
          
          <code><a href="torch_layout.html">torch_strided()</a></code> <code><a href="torch_layout.html">torch_sparse_coo()</a></code> 
        </dt>
        <dd>Creates the corresponding layout</dd>
      </dl><dl><dt>
          
          <code><a href="torch_lcm.html">torch_lcm()</a></code> 
        </dt>
        <dd>Lcm</dd>
      </dl><dl><dt>
          
          <code><a href="torch_le.html">torch_le()</a></code> 
        </dt>
        <dd>Le</dd>
      </dl><dl><dt>
          
          <code><a href="torch_lerp.html">torch_lerp()</a></code> 
        </dt>
        <dd>Lerp</dd>
      </dl><dl><dt>
          
          <code><a href="torch_less.html">torch_less()</a></code> 
        </dt>
        <dd>Less</dd>
      </dl><dl><dt>
          
          <code><a href="torch_less_equal.html">torch_less_equal()</a></code> 
        </dt>
        <dd>Less_equal</dd>
      </dl><dl><dt>
          
          <code><a href="torch_lgamma.html">torch_lgamma()</a></code> 
        </dt>
        <dd>Lgamma</dd>
      </dl><dl><dt>
          
          <code><a href="torch_log.html">torch_log()</a></code> 
        </dt>
        <dd>Log</dd>
      </dl><dl><dt>
          
          <code><a href="torch_log10.html">torch_log10()</a></code> 
        </dt>
        <dd>Log10</dd>
      </dl><dl><dt>
          
          <code><a href="torch_log1p.html">torch_log1p()</a></code> 
        </dt>
        <dd>Log1p</dd>
      </dl><dl><dt>
          
          <code><a href="torch_log2.html">torch_log2()</a></code> 
        </dt>
        <dd>Log2</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logaddexp.html">torch_logaddexp()</a></code> 
        </dt>
        <dd>Logaddexp</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logaddexp2.html">torch_logaddexp2()</a></code> 
        </dt>
        <dd>Logaddexp2</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logcumsumexp.html">torch_logcumsumexp()</a></code> 
        </dt>
        <dd>Logcumsumexp</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logdet.html">torch_logdet()</a></code> 
        </dt>
        <dd>Logdet</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logical_and.html">torch_logical_and()</a></code> 
        </dt>
        <dd>Logical_and</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logical_not.html">torch_logical_not</a></code> 
        </dt>
        <dd>Logical_not</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logical_or.html">torch_logical_or()</a></code> 
        </dt>
        <dd>Logical_or</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logical_xor.html">torch_logical_xor()</a></code> 
        </dt>
        <dd>Logical_xor</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logit.html">torch_logit()</a></code> 
        </dt>
        <dd>Logit</dd>
      </dl><dl><dt>
          
          <code><a href="torch_logsumexp.html">torch_logsumexp()</a></code> 
        </dt>
        <dd>Logsumexp</dd>
      </dl><dl><dt>
          
          <code><a href="torch_lstsq.html">torch_lstsq</a></code> 
        </dt>
        <dd>Lstsq</dd>
      </dl><dl><dt>
          
          <code><a href="torch_lt.html">torch_lt()</a></code> 
        </dt>
        <dd>Lt</dd>
      </dl><dl><dt>
          
          <code><a href="torch_lu.html">torch_lu()</a></code> 
        </dt>
        <dd>LU</dd>
      </dl><dl><dt>
          
          <code><a href="torch_lu_solve.html">torch_lu_solve()</a></code> 
        </dt>
        <dd>Lu_solve</dd>
      </dl><dl><dt>
          
          <code><a href="torch_lu_unpack.html">torch_lu_unpack()</a></code> 
        </dt>
        <dd>Lu_unpack</dd>
      </dl><dl><dt>
          
          <code><a href="torch_manual_seed.html">torch_manual_seed()</a></code> <code><a href="torch_manual_seed.html">local_torch_manual_seed()</a></code> <code><a href="torch_manual_seed.html">with_torch_manual_seed()</a></code> 
        </dt>
        <dd>Sets the seed for generating random numbers.</dd>
      </dl><dl><dt>
          
          <code><a href="torch_masked_select.html">torch_masked_select()</a></code> 
        </dt>
        <dd>Masked_select</dd>
      </dl><dl><dt>
          
          <code><a href="torch_matmul.html">torch_matmul()</a></code> 
        </dt>
        <dd>Matmul</dd>
      </dl><dl><dt>
          
          <code><a href="torch_matrix_exp.html">torch_matrix_exp()</a></code> 
        </dt>
        <dd>Matrix_exp</dd>
      </dl><dl><dt>
          
          <code><a href="torch_matrix_power.html">torch_matrix_power()</a></code> 
        </dt>
        <dd>Matrix_power</dd>
      </dl><dl><dt>
          
          <code><a href="torch_matrix_rank.html">torch_matrix_rank</a></code> 
        </dt>
        <dd>Matrix_rank</dd>
      </dl><dl><dt>
          
          <code><a href="torch_max.html">torch_max</a></code> 
        </dt>
        <dd>Max</dd>
      </dl><dl><dt>
          
          <code><a href="torch_maximum.html">torch_maximum()</a></code> 
        </dt>
        <dd>Maximum</dd>
      </dl><dl><dt>
          
          <code><a href="torch_mean.html">torch_mean()</a></code> 
        </dt>
        <dd>Mean</dd>
      </dl><dl><dt>
          
          <code><a href="torch_median.html">torch_median()</a></code> 
        </dt>
        <dd>Median</dd>
      </dl><dl><dt>
          
          <code><a href="torch_memory_format.html">torch_contiguous_format()</a></code> <code><a href="torch_memory_format.html">torch_preserve_format()</a></code> <code><a href="torch_memory_format.html">torch_channels_last_format()</a></code> 
        </dt>
        <dd>Memory format</dd>
      </dl><dl><dt>
          
          <code><a href="torch_meshgrid.html">torch_meshgrid()</a></code> 
        </dt>
        <dd>Meshgrid</dd>
      </dl><dl><dt>
          
          <code><a href="torch_min.html">torch_min</a></code> 
        </dt>
        <dd>Min</dd>
      </dl><dl><dt>
          
          <code><a href="torch_minimum.html">torch_minimum()</a></code> 
        </dt>
        <dd>Minimum</dd>
      </dl><dl><dt>
          
          <code><a href="torch_mm.html">torch_mm()</a></code> 
        </dt>
        <dd>Mm</dd>
      </dl><dl><dt>
          
          <code><a href="torch_mode.html">torch_mode()</a></code> 
        </dt>
        <dd>Mode</dd>
      </dl><dl><dt>
          
          <code><a href="torch_movedim.html">torch_movedim()</a></code> 
        </dt>
        <dd>Movedim</dd>
      </dl><dl><dt>
          
          <code><a href="torch_mul.html">torch_mul()</a></code> 
        </dt>
        <dd>Mul</dd>
      </dl><dl><dt>
          
          <code><a href="torch_multinomial.html">torch_multinomial()</a></code> 
        </dt>
        <dd>Multinomial</dd>
      </dl><dl><dt>
          
          <code><a href="torch_multiply.html">torch_multiply()</a></code> 
        </dt>
        <dd>Multiply</dd>
      </dl><dl><dt>
          
          <code><a href="torch_mv.html">torch_mv()</a></code> 
        </dt>
        <dd>Mv</dd>
      </dl><dl><dt>
          
          <code><a href="torch_mvlgamma.html">torch_mvlgamma()</a></code> 
        </dt>
        <dd>Mvlgamma</dd>
      </dl><dl><dt>
          
          <code><a href="torch_nanquantile.html">torch_nanquantile()</a></code> 
        </dt>
        <dd>Nanquantile</dd>
      </dl><dl><dt>
          
          <code><a href="torch_nansum.html">torch_nansum()</a></code> 
        </dt>
        <dd>Nansum</dd>
      </dl><dl><dt>
          
          <code><a href="torch_narrow.html">torch_narrow()</a></code> 
        </dt>
        <dd>Narrow</dd>
      </dl><dl><dt>
          
          <code><a href="torch_ne.html">torch_ne()</a></code> 
        </dt>
        <dd>Ne</dd>
      </dl><dl><dt>
          
          <code><a href="torch_neg.html">torch_neg()</a></code> 
        </dt>
        <dd>Neg</dd>
      </dl><dl><dt>
          
          <code><a href="torch_negative.html">torch_negative()</a></code> 
        </dt>
        <dd>Negative</dd>
      </dl><dl><dt>
          
          <code><a href="torch_nextafter.html">torch_nextafter()</a></code> 
        </dt>
        <dd>Nextafter</dd>
      </dl><dl><dt>
          
          <code><a href="torch_nonzero.html">torch_nonzero()</a></code> 
        </dt>
        <dd>Nonzero</dd>
      </dl><dl><dt>
          
          <code><a href="torch_norm.html">torch_norm()</a></code> 
        </dt>
        <dd>Norm</dd>
      </dl><dl><dt>
          
          <code><a href="torch_normal.html">torch_normal()</a></code> 
        </dt>
        <dd>Normal</dd>
      </dl><dl><dt>
          
          <code><a href="torch_not_equal.html">torch_not_equal()</a></code> 
        </dt>
        <dd>Not_equal</dd>
      </dl><dl><dt>
          
          <code><a href="torch_orgqr.html">torch_orgqr()</a></code> 
        </dt>
        <dd>Orgqr</dd>
      </dl><dl><dt>
          
          <code><a href="torch_ormqr.html">torch_ormqr()</a></code> 
        </dt>
        <dd>Ormqr</dd>
      </dl><dl><dt>
          
          <code><a href="torch_outer.html">torch_outer()</a></code> 
        </dt>
        <dd>Outer</dd>
      </dl><dl><dt>
          
          <code><a href="torch_pdist.html">torch_pdist()</a></code> 
        </dt>
        <dd>Pdist</dd>
      </dl><dl><dt>
          
          <code><a href="torch_pinverse.html">torch_pinverse()</a></code> 
        </dt>
        <dd>Pinverse</dd>
      </dl><dl><dt>
          
          <code><a href="torch_pixel_shuffle.html">torch_pixel_shuffle()</a></code> 
        </dt>
        <dd>Pixel_shuffle</dd>
      </dl><dl><dt>
          
          <code><a href="torch_poisson.html">torch_poisson()</a></code> 
        </dt>
        <dd>Poisson</dd>
      </dl><dl><dt>
          
          <code><a href="torch_polar.html">torch_polar()</a></code> 
        </dt>
        <dd>Polar</dd>
      </dl><dl><dt>
          
          <code><a href="torch_polygamma.html">torch_polygamma()</a></code> 
        </dt>
        <dd>Polygamma</dd>
      </dl><dl><dt>
          
          <code><a href="torch_pow.html">torch_pow()</a></code> 
        </dt>
        <dd>Pow</dd>
      </dl><dl><dt>
          
          <code><a href="torch_prod.html">torch_prod()</a></code> 
        </dt>
        <dd>Prod</dd>
      </dl><dl><dt>
          
          <code><a href="torch_promote_types.html">torch_promote_types()</a></code> 
        </dt>
        <dd>Promote_types</dd>
      </dl><dl><dt>
          
          <code><a href="torch_qr.html">torch_qr()</a></code> 
        </dt>
        <dd>Qr</dd>
      </dl><dl><dt>
          
          <code><a href="torch_quantile.html">torch_quantile()</a></code> 
        </dt>
        <dd>Quantile</dd>
      </dl><dl><dt>
          
          <code><a href="torch_quantize_per_channel.html">torch_quantize_per_channel()</a></code> 
        </dt>
        <dd>Quantize_per_channel</dd>
      </dl><dl><dt>
          
          <code><a href="torch_quantize_per_tensor.html">torch_quantize_per_tensor()</a></code> 
        </dt>
        <dd>Quantize_per_tensor</dd>
      </dl><dl><dt>
          
          <code><a href="torch_rad2deg.html">torch_rad2deg()</a></code> 
        </dt>
        <dd>Rad2deg</dd>
      </dl><dl><dt>
          
          <code><a href="torch_range.html">torch_range()</a></code> 
        </dt>
        <dd>Range</dd>
      </dl><dl><dt>
          
          <code><a href="torch_real.html">torch_real()</a></code> 
        </dt>
        <dd>Real</dd>
      </dl><dl><dt>
          
          <code><a href="torch_reciprocal.html">torch_reciprocal()</a></code> 
        </dt>
        <dd>Reciprocal</dd>
      </dl><dl><dt>
          
          <code><a href="torch_relu.html">torch_relu()</a></code> 
        </dt>
        <dd>Relu</dd>
      </dl><dl><dt>
          
          <code><a href="torch_relu_.html">torch_relu_()</a></code> 
        </dt>
        <dd>Relu_</dd>
      </dl><dl><dt>
          
          <code><a href="torch_remainder.html">torch_remainder()</a></code> 
        </dt>
        <dd>Remainder</dd>
      </dl><dl><dt>
          
          <code><a href="torch_renorm.html">torch_renorm()</a></code> 
        </dt>
        <dd>Renorm</dd>
      </dl><dl><dt>
          
          <code><a href="torch_repeat_interleave.html">torch_repeat_interleave()</a></code> 
        </dt>
        <dd>Repeat_interleave</dd>
      </dl><dl><dt>
          
          <code><a href="torch_reshape.html">torch_reshape()</a></code> 
        </dt>
        <dd>Reshape</dd>
      </dl><dl><dt>
          
          <code><a href="torch_result_type.html">torch_result_type()</a></code> 
        </dt>
        <dd>Result_type</dd>
      </dl><dl><dt>
          
          <code><a href="torch_roll.html">torch_roll()</a></code> 
        </dt>
        <dd>Roll</dd>
      </dl><dl><dt>
          
          <code><a href="torch_rot90.html">torch_rot90()</a></code> 
        </dt>
        <dd>Rot90</dd>
      </dl><dl><dt>
          
          <code><a href="torch_round.html">torch_round()</a></code> 
        </dt>
        <dd>Round</dd>
      </dl><dl><dt>
          
          <code><a href="torch_rrelu_.html">torch_rrelu_()</a></code> 
        </dt>
        <dd>Rrelu_</dd>
      </dl><dl><dt>
          
          <code><a href="torch_rsqrt.html">torch_rsqrt()</a></code> 
        </dt>
        <dd>Rsqrt</dd>
      </dl><dl><dt>
          
          <code><a href="torch_scalar_tensor.html">torch_scalar_tensor()</a></code> 
        </dt>
        <dd>Scalar tensor</dd>
      </dl><dl><dt>
          
          <code><a href="torch_searchsorted.html">torch_searchsorted()</a></code> 
        </dt>
        <dd>Searchsorted</dd>
      </dl><dl><dt>
          
          <code><a href="torch_selu.html">torch_selu()</a></code> 
        </dt>
        <dd>Selu</dd>
      </dl><dl><dt>
          
          <code><a href="torch_selu_.html">torch_selu_()</a></code> 
        </dt>
        <dd>Selu_</dd>
      </dl><dl><dt>
          
          <code><a href="torch_sgn.html">torch_sgn()</a></code> 
        </dt>
        <dd>Sgn</dd>
      </dl><dl><dt>
          
          <code><a href="torch_sigmoid.html">torch_sigmoid()</a></code> 
        </dt>
        <dd>Sigmoid</dd>
      </dl><dl><dt>
          
          <code><a href="torch_sign.html">torch_sign()</a></code> 
        </dt>
        <dd>Sign</dd>
      </dl><dl><dt>
          
          <code><a href="torch_signbit.html">torch_signbit()</a></code> 
        </dt>
        <dd>Signbit</dd>
      </dl><dl><dt>
          
          <code><a href="torch_sin.html">torch_sin()</a></code> 
        </dt>
        <dd>Sin</dd>
      </dl><dl><dt>
          
          <code><a href="torch_sinh.html">torch_sinh()</a></code> 
        </dt>
        <dd>Sinh</dd>
      </dl><dl><dt>
          
          <code><a href="torch_slogdet.html">torch_slogdet()</a></code> 
        </dt>
        <dd>Slogdet</dd>
      </dl><dl><dt>
          
          <code><a href="torch_sort.html">torch_sort</a></code> 
        </dt>
        <dd>Sort</dd>
      </dl><dl><dt>
          
          <code><a href="torch_sparse_coo_tensor.html">torch_sparse_coo_tensor()</a></code> 
        </dt>
        <dd>Sparse_coo_tensor</dd>
      </dl><dl><dt>
          
          <code><a href="torch_split.html">torch_split()</a></code> 
        </dt>
        <dd>Split</dd>
      </dl><dl><dt>
          
          <code><a href="torch_sqrt.html">torch_sqrt()</a></code> 
        </dt>
        <dd>Sqrt</dd>
      </dl><dl><dt>
          
          <code><a href="torch_square.html">torch_square()</a></code> 
        </dt>
        <dd>Square</dd>
      </dl><dl><dt>
          
          <code><a href="torch_squeeze.html">torch_squeeze()</a></code> 
        </dt>
        <dd>Squeeze</dd>
      </dl><dl><dt>
          
          <code><a href="torch_stack.html">torch_stack()</a></code> 
        </dt>
        <dd>Stack</dd>
      </dl><dl><dt>
          
          <code><a href="torch_std.html">torch_std()</a></code> 
        </dt>
        <dd>Std</dd>
      </dl><dl><dt>
          
          <code><a href="torch_std_mean.html">torch_std_mean()</a></code> 
        </dt>
        <dd>Std_mean</dd>
      </dl><dl><dt>
          
          <code><a href="torch_stft.html">torch_stft()</a></code> 
        </dt>
        <dd>Stft</dd>
      </dl><dl><dt>
          
          <code><a href="torch_sub.html">torch_sub()</a></code> 
        </dt>
        <dd>Sub</dd>
      </dl><dl><dt>
          
          <code><a href="torch_subtract.html">torch_subtract()</a></code> 
        </dt>
        <dd>Subtract</dd>
      </dl><dl><dt>
          
          <code><a href="torch_sum.html">torch_sum()</a></code> 
        </dt>
        <dd>Sum</dd>
      </dl><dl><dt>
          
          <code><a href="torch_svd.html">torch_svd()</a></code> 
        </dt>
        <dd>Svd</dd>
      </dl><dl><dt>
          
          <code><a href="torch_t.html">torch_t()</a></code> 
        </dt>
        <dd>T</dd>
      </dl><dl><dt>
          
          <code><a href="torch_take.html">torch_take()</a></code> 
        </dt>
        <dd>Take</dd>
      </dl><dl><dt>
          
          <code><a href="torch_tan.html">torch_tan()</a></code> 
        </dt>
        <dd>Tan</dd>
      </dl><dl><dt>
          
          <code><a href="torch_tanh.html">torch_tanh()</a></code> 
        </dt>
        <dd>Tanh</dd>
      </dl><dl><dt>
          
          <code><a href="torch_tensor.html">torch_tensor()</a></code> 
        </dt>
        <dd>Converts R objects to a torch tensor</dd>
      </dl><dl><dt>
          
          <code><a href="torch_tensor_from_buffer.html">torch_tensor_from_buffer()</a></code> <code><a href="torch_tensor_from_buffer.html">buffer_from_torch_tensor()</a></code> 
        </dt>
        <dd>Creates a tensor from a buffer of memory</dd>
      </dl><dl><dt>
          
          <code><a href="torch_tensordot.html">torch_tensordot()</a></code> 
        </dt>
        <dd>Tensordot</dd>
      </dl><dl><dt>
          
          <code><a href="torch_threshold_.html">torch_threshold_()</a></code> 
        </dt>
        <dd>Threshold_</dd>
      </dl><dl><dt>
          
          <code><a href="torch_topk.html">torch_topk()</a></code> 
        </dt>
        <dd>Topk</dd>
      </dl><dl><dt>
          
          <code><a href="torch_trace.html">torch_trace()</a></code> 
        </dt>
        <dd>Trace</dd>
      </dl><dl><dt>
          
          <code><a href="torch_transpose.html">torch_transpose()</a></code> 
        </dt>
        <dd>Transpose</dd>
      </dl><dl><dt>
          
          <code><a href="torch_trapz.html">torch_trapz()</a></code> 
        </dt>
        <dd>Trapz</dd>
      </dl><dl><dt>
          
          <code><a href="torch_triangular_solve.html">torch_triangular_solve()</a></code> 
        </dt>
        <dd>Triangular_solve</dd>
      </dl><dl><dt>
          
          <code><a href="torch_tril.html">torch_tril()</a></code> 
        </dt>
        <dd>Tril</dd>
      </dl><dl><dt>
          
          <code><a href="torch_tril_indices.html">torch_tril_indices()</a></code> 
        </dt>
        <dd>Tril_indices</dd>
      </dl><dl><dt>
          
          <code><a href="torch_triu.html">torch_triu()</a></code> 
        </dt>
        <dd>Triu</dd>
      </dl><dl><dt>
          
          <code><a href="torch_triu_indices.html">torch_triu_indices()</a></code> 
        </dt>
        <dd>Triu_indices</dd>
      </dl><dl><dt>
          
          <code><a href="torch_true_divide.html">torch_true_divide()</a></code> 
        </dt>
        <dd>TRUE_divide</dd>
      </dl><dl><dt>
          
          <code><a href="torch_trunc.html">torch_trunc()</a></code> 
        </dt>
        <dd>Trunc</dd>
      </dl><dl><dt>
          
          <code><a href="torch_unbind.html">torch_unbind()</a></code> 
        </dt>
        <dd>Unbind</dd>
      </dl><dl><dt>
          
          <code><a href="torch_unique_consecutive.html">torch_unique_consecutive()</a></code> 
        </dt>
        <dd>Unique_consecutive</dd>
      </dl><dl><dt>
          
          <code><a href="torch_unsafe_chunk.html">torch_unsafe_chunk()</a></code> 
        </dt>
        <dd>Unsafe_chunk</dd>
      </dl><dl><dt>
          
          <code><a href="torch_unsafe_split.html">torch_unsafe_split()</a></code> 
        </dt>
        <dd>Unsafe_split</dd>
      </dl><dl><dt>
          
          <code><a href="torch_unsqueeze.html">torch_unsqueeze()</a></code> 
        </dt>
        <dd>Unsqueeze</dd>
      </dl><dl><dt>
          
          <code><a href="torch_vander.html">torch_vander()</a></code> 
        </dt>
        <dd>Vander</dd>
      </dl><dl><dt>
          
          <code><a href="torch_var.html">torch_var()</a></code> 
        </dt>
        <dd>Var</dd>
      </dl><dl><dt>
          
          <code><a href="torch_var_mean.html">torch_var_mean()</a></code> 
        </dt>
        <dd>Var_mean</dd>
      </dl><dl><dt>
          
          <code><a href="torch_vdot.html">torch_vdot()</a></code> 
        </dt>
        <dd>Vdot</dd>
      </dl><dl><dt>
          
          <code><a href="torch_view_as_complex.html">torch_view_as_complex()</a></code> 
        </dt>
        <dd>View_as_complex</dd>
      </dl><dl><dt>
          
          <code><a href="torch_view_as_real.html">torch_view_as_real()</a></code> 
        </dt>
        <dd>View_as_real</dd>
      </dl><dl><dt>
          
          <code><a href="torch_vstack.html">torch_vstack()</a></code> 
        </dt>
        <dd>Vstack</dd>
      </dl><dl><dt>
          
          <code><a href="torch_where.html">torch_where()</a></code> 
        </dt>
        <dd>Where</dd>
      </dl><dl><dt>
          
          <code><a href="broadcast_all.html">broadcast_all()</a></code> 
        </dt>
        <dd>Given a list of values (possibly containing numbers), returns a list where each value is broadcasted based on the following rules:</dd>
      </dl></div><div class="section level2">
      <h2 id="neural-network-modules">Neural network modules<a class="anchor" aria-label="anchor" href="#neural-network-modules"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="nn_adaptive_avg_pool1d.html">nn_adaptive_avg_pool1d()</a></code> 
        </dt>
        <dd>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_adaptive_avg_pool2d.html">nn_adaptive_avg_pool2d()</a></code> 
        </dt>
        <dd>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_adaptive_avg_pool3d.html">nn_adaptive_avg_pool3d()</a></code> 
        </dt>
        <dd>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_adaptive_log_softmax_with_loss.html">nn_adaptive_log_softmax_with_loss()</a></code> 
        </dt>
        <dd>AdaptiveLogSoftmaxWithLoss module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_adaptive_max_pool1d.html">nn_adaptive_max_pool1d()</a></code> 
        </dt>
        <dd>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_adaptive_max_pool2d.html">nn_adaptive_max_pool2d()</a></code> 
        </dt>
        <dd>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_adaptive_max_pool3d.html">nn_adaptive_max_pool3d()</a></code> 
        </dt>
        <dd>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_avg_pool1d.html">nn_avg_pool1d()</a></code> 
        </dt>
        <dd>Applies a 1D average pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_avg_pool2d.html">nn_avg_pool2d()</a></code> 
        </dt>
        <dd>Applies a 2D average pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_avg_pool3d.html">nn_avg_pool3d()</a></code> 
        </dt>
        <dd>Applies a 3D average pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_batch_norm1d.html">nn_batch_norm1d()</a></code> 
        </dt>
        <dd>BatchNorm1D module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_batch_norm2d.html">nn_batch_norm2d()</a></code> 
        </dt>
        <dd>BatchNorm2D</dd>
      </dl><dl><dt>
          
          <code><a href="nn_batch_norm3d.html">nn_batch_norm3d()</a></code> 
        </dt>
        <dd>BatchNorm3D</dd>
      </dl><dl><dt>
          
          <code><a href="nn_bce_loss.html">nn_bce_loss()</a></code> 
        </dt>
        <dd>Binary cross entropy loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_bce_with_logits_loss.html">nn_bce_with_logits_loss()</a></code> 
        </dt>
        <dd>BCE with logits loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_bilinear.html">nn_bilinear()</a></code> 
        </dt>
        <dd>Bilinear module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_buffer.html">nn_buffer()</a></code> 
        </dt>
        <dd>Creates a nn_buffer</dd>
      </dl><dl><dt>
          
          <code><a href="nn_celu.html">nn_celu()</a></code> 
        </dt>
        <dd>CELU module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_contrib_sparsemax.html">nn_contrib_sparsemax()</a></code> 
        </dt>
        <dd>Sparsemax activation</dd>
      </dl><dl><dt>
          
          <code><a href="nn_conv1d.html">nn_conv1d()</a></code> 
        </dt>
        <dd>Conv1D module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_conv2d.html">nn_conv2d()</a></code> 
        </dt>
        <dd>Conv2D module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_conv3d.html">nn_conv3d()</a></code> 
        </dt>
        <dd>Conv3D module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_conv_transpose1d.html">nn_conv_transpose1d()</a></code> 
        </dt>
        <dd>ConvTranspose1D</dd>
      </dl><dl><dt>
          
          <code><a href="nn_conv_transpose2d.html">nn_conv_transpose2d()</a></code> 
        </dt>
        <dd>ConvTranpose2D module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_conv_transpose3d.html">nn_conv_transpose3d()</a></code> 
        </dt>
        <dd>ConvTranpose3D module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_cosine_embedding_loss.html">nn_cosine_embedding_loss()</a></code> 
        </dt>
        <dd>Cosine embedding loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_cross_entropy_loss.html">nn_cross_entropy_loss()</a></code> 
        </dt>
        <dd>CrossEntropyLoss module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_ctc_loss.html">nn_ctc_loss()</a></code> 
        </dt>
        <dd>The Connectionist Temporal Classification loss.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_dropout.html">nn_dropout()</a></code> 
        </dt>
        <dd>Dropout module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_dropout2d.html">nn_dropout2d()</a></code> 
        </dt>
        <dd>Dropout2D module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_dropout3d.html">nn_dropout3d()</a></code> 
        </dt>
        <dd>Dropout3D module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_elu.html">nn_elu()</a></code> 
        </dt>
        <dd>ELU module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_embedding.html">nn_embedding()</a></code> 
        </dt>
        <dd>Embedding module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_embedding_bag.html">nn_embedding_bag()</a></code> 
        </dt>
        <dd>Embedding bag module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_flatten.html">nn_flatten()</a></code> 
        </dt>
        <dd>Flattens a contiguous range of dims into a tensor.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_fractional_max_pool2d.html">nn_fractional_max_pool2d()</a></code> 
        </dt>
        <dd>Applies a 2D fractional max pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_fractional_max_pool3d.html">nn_fractional_max_pool3d()</a></code> 
        </dt>
        <dd>Applies a 3D fractional max pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_gelu.html">nn_gelu()</a></code> 
        </dt>
        <dd>GELU module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_glu.html">nn_glu()</a></code> 
        </dt>
        <dd>GLU module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_group_norm.html">nn_group_norm()</a></code> 
        </dt>
        <dd>Group normalization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_gru.html">nn_gru()</a></code> 
        </dt>
        <dd>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_hardshrink.html">nn_hardshrink()</a></code> 
        </dt>
        <dd>Hardshwink module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_hardsigmoid.html">nn_hardsigmoid()</a></code> 
        </dt>
        <dd>Hardsigmoid module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_hardswish.html">nn_hardswish()</a></code> 
        </dt>
        <dd>Hardswish module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_hardtanh.html">nn_hardtanh()</a></code> 
        </dt>
        <dd>Hardtanh module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_hinge_embedding_loss.html">nn_hinge_embedding_loss()</a></code> 
        </dt>
        <dd>Hinge embedding loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_identity.html">nn_identity()</a></code> 
        </dt>
        <dd>Identity module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_calculate_gain.html">nn_init_calculate_gain()</a></code> 
        </dt>
        <dd>Calculate gain</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_constant_.html">nn_init_constant_()</a></code> 
        </dt>
        <dd>Constant initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_dirac_.html">nn_init_dirac_()</a></code> 
        </dt>
        <dd>Dirac initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_eye_.html">nn_init_eye_()</a></code> 
        </dt>
        <dd>Eye initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_kaiming_normal_.html">nn_init_kaiming_normal_()</a></code> 
        </dt>
        <dd>Kaiming normal initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_kaiming_uniform_.html">nn_init_kaiming_uniform_()</a></code> 
        </dt>
        <dd>Kaiming uniform initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_normal_.html">nn_init_normal_()</a></code> 
        </dt>
        <dd>Normal initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_ones_.html">nn_init_ones_()</a></code> 
        </dt>
        <dd>Ones initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_orthogonal_.html">nn_init_orthogonal_()</a></code> 
        </dt>
        <dd>Orthogonal initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_sparse_.html">nn_init_sparse_()</a></code> 
        </dt>
        <dd>Sparse initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_trunc_normal_.html">nn_init_trunc_normal_()</a></code> 
        </dt>
        <dd>Truncated normal initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_uniform_.html">nn_init_uniform_()</a></code> 
        </dt>
        <dd>Uniform initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_xavier_normal_.html">nn_init_xavier_normal_()</a></code> 
        </dt>
        <dd>Xavier normal initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_xavier_uniform_.html">nn_init_xavier_uniform_()</a></code> 
        </dt>
        <dd>Xavier uniform initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_init_zeros_.html">nn_init_zeros_()</a></code> 
        </dt>
        <dd>Zeros initialization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_kl_div_loss.html">nn_kl_div_loss()</a></code> 
        </dt>
        <dd>Kullback-Leibler divergence loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_l1_loss.html">nn_l1_loss()</a></code> 
        </dt>
        <dd>L1 loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_layer_norm.html">nn_layer_norm()</a></code> 
        </dt>
        <dd>Layer normalization</dd>
      </dl><dl><dt>
          
          <code><a href="nn_leaky_relu.html">nn_leaky_relu()</a></code> 
        </dt>
        <dd>LeakyReLU module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_linear.html">nn_linear()</a></code> 
        </dt>
        <dd>Linear module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_log_sigmoid.html">nn_log_sigmoid()</a></code> 
        </dt>
        <dd>LogSigmoid module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_log_softmax.html">nn_log_softmax()</a></code> 
        </dt>
        <dd>LogSoftmax module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_lp_pool1d.html">nn_lp_pool1d()</a></code> 
        </dt>
        <dd>Applies a 1D power-average pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_lp_pool2d.html">nn_lp_pool2d()</a></code> 
        </dt>
        <dd>Applies a 2D power-average pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_lstm.html">nn_lstm()</a></code> 
        </dt>
        <dd>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_margin_ranking_loss.html">nn_margin_ranking_loss()</a></code> 
        </dt>
        <dd>Margin ranking loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_max_pool1d.html">nn_max_pool1d()</a></code> 
        </dt>
        <dd>MaxPool1D module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_max_pool2d.html">nn_max_pool2d()</a></code> 
        </dt>
        <dd>MaxPool2D module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_max_pool3d.html">nn_max_pool3d()</a></code> 
        </dt>
        <dd>Applies a 3D max pooling over an input signal composed of several input planes.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_max_unpool1d.html">nn_max_unpool1d()</a></code> 
        </dt>
        <dd>Computes a partial inverse of <code>MaxPool1d</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_max_unpool2d.html">nn_max_unpool2d()</a></code> 
        </dt>
        <dd>Computes a partial inverse of <code>MaxPool2d</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_max_unpool3d.html">nn_max_unpool3d()</a></code> 
        </dt>
        <dd>Computes a partial inverse of <code>MaxPool3d</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_module.html">nn_module()</a></code> 
        </dt>
        <dd>Base class for all neural network modules.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_module_dict.html">nn_module_dict()</a></code> 
        </dt>
        <dd>Container that allows named values</dd>
      </dl><dl><dt>
          
          <code><a href="nn_module_list.html">nn_module_list()</a></code> 
        </dt>
        <dd>Holds submodules in a list.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_mse_loss.html">nn_mse_loss()</a></code> 
        </dt>
        <dd>MSE loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_multi_margin_loss.html">nn_multi_margin_loss()</a></code> 
        </dt>
        <dd>Multi margin loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_multihead_attention.html">nn_multihead_attention()</a></code> 
        </dt>
        <dd>MultiHead attention</dd>
      </dl><dl><dt>
          
          <code><a href="nn_multilabel_margin_loss.html">nn_multilabel_margin_loss()</a></code> 
        </dt>
        <dd>Multilabel margin loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_multilabel_soft_margin_loss.html">nn_multilabel_soft_margin_loss()</a></code> 
        </dt>
        <dd>Multi label soft margin loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_nll_loss.html">nn_nll_loss()</a></code> 
        </dt>
        <dd>Nll loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_pairwise_distance.html">nn_pairwise_distance()</a></code> 
        </dt>
        <dd>Pairwise distance</dd>
      </dl><dl><dt>
          
          <code><a href="nn_parameter.html">nn_parameter()</a></code> 
        </dt>
        <dd>Creates an <code>nn_parameter</code></dd>
      </dl><dl><dt>
          
          <code><a href="nn_poisson_nll_loss.html">nn_poisson_nll_loss()</a></code> 
        </dt>
        <dd>Poisson NLL loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_prelu.html">nn_prelu()</a></code> 
        </dt>
        <dd>PReLU module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_prune_head.html">nn_prune_head()</a></code> 
        </dt>
        <dd>Prune top layer(s) of a network</dd>
      </dl><dl><dt>
          
          <code><a href="nn_relu.html">nn_relu()</a></code> 
        </dt>
        <dd>ReLU module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_relu6.html">nn_relu6()</a></code> 
        </dt>
        <dd>ReLu6 module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_rnn.html">nn_rnn()</a></code> 
        </dt>
        <dd>RNN module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_rrelu.html">nn_rrelu()</a></code> 
        </dt>
        <dd>RReLU module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_selu.html">nn_selu()</a></code> 
        </dt>
        <dd>SELU module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_sequential.html">nn_sequential()</a></code> 
        </dt>
        <dd>A sequential container</dd>
      </dl><dl><dt>
          
          <code><a href="nn_sigmoid.html">nn_sigmoid()</a></code> 
        </dt>
        <dd>Sigmoid module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_silu.html">nn_silu()</a></code> 
        </dt>
        <dd>Applies the Sigmoid Linear Unit (SiLU) function, element-wise. The SiLU function is also known as the swish function.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_smooth_l1_loss.html">nn_smooth_l1_loss()</a></code> 
        </dt>
        <dd>Smooth L1 loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_soft_margin_loss.html">nn_soft_margin_loss()</a></code> 
        </dt>
        <dd>Soft margin loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_softmax.html">nn_softmax()</a></code> 
        </dt>
        <dd>Softmax module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_softmax2d.html">nn_softmax2d()</a></code> 
        </dt>
        <dd>Softmax2d module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_softmin.html">nn_softmin()</a></code> 
        </dt>
        <dd>Softmin</dd>
      </dl><dl><dt>
          
          <code><a href="nn_softplus.html">nn_softplus()</a></code> 
        </dt>
        <dd>Softplus module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_softshrink.html">nn_softshrink()</a></code> 
        </dt>
        <dd>Softshrink module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_softsign.html">nn_softsign()</a></code> 
        </dt>
        <dd>Softsign module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_tanh.html">nn_tanh()</a></code> 
        </dt>
        <dd>Tanh module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_tanhshrink.html">nn_tanhshrink()</a></code> 
        </dt>
        <dd>Tanhshrink module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_threshold.html">nn_threshold()</a></code> 
        </dt>
        <dd>Threshold module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_triplet_margin_loss.html">nn_triplet_margin_loss()</a></code> 
        </dt>
        <dd>Triplet margin loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_triplet_margin_with_distance_loss.html">nn_triplet_margin_with_distance_loss()</a></code> 
        </dt>
        <dd>Triplet margin with distance loss</dd>
      </dl><dl><dt>
          
          <code><a href="nn_unflatten.html">nn_unflatten()</a></code> 
        </dt>
        <dd>Unflattens a tensor dim expanding it to a desired shape. For use with [nn_sequential.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_upsample.html">nn_upsample()</a></code> 
        </dt>
        <dd>Upsample module</dd>
      </dl><dl><dt>
          
          <code><a href="nn_utils_clip_grad_norm_.html">nn_utils_clip_grad_norm_()</a></code> 
        </dt>
        <dd>Clips gradient norm of an iterable of parameters.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_utils_clip_grad_value_.html">nn_utils_clip_grad_value_()</a></code> 
        </dt>
        <dd>Clips gradient of an iterable of parameters at specified value.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_utils_rnn_pack_padded_sequence.html">nn_utils_rnn_pack_padded_sequence()</a></code> 
        </dt>
        <dd>Packs a Tensor containing padded sequences of variable length.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_utils_rnn_pack_sequence.html">nn_utils_rnn_pack_sequence()</a></code> 
        </dt>
        <dd>Packs a list of variable length Tensors</dd>
      </dl><dl><dt>
          
          <code><a href="nn_utils_rnn_pad_packed_sequence.html">nn_utils_rnn_pad_packed_sequence()</a></code> 
        </dt>
        <dd>Pads a packed batch of variable length sequences.</dd>
      </dl><dl><dt>
          
          <code><a href="nn_utils_rnn_pad_sequence.html">nn_utils_rnn_pad_sequence()</a></code> 
        </dt>
        <dd>Pad a list of variable length Tensors with <code>padding_value</code></dd>
      </dl><dl><dt>
          
          <code><a href="nn_utils_weight_norm.html">nn_utils_weight_norm</a></code> 
        </dt>
        <dd>nn_utils_weight_norm</dd>
      </dl><dl><dt>
          
          <code><a href="is_nn_module.html">is_nn_module()</a></code> 
        </dt>
        <dd>Checks if the object is an nn_module</dd>
      </dl><dl><dt>
          
          <code><a href="is_nn_parameter.html">is_nn_parameter()</a></code> 
        </dt>
        <dd>Checks if an object is a nn_parameter</dd>
      </dl><dl><dt>
          
          <code><a href="is_nn_buffer.html">is_nn_buffer()</a></code> 
        </dt>
        <dd>Checks if the object is a nn_buffer</dd>
      </dl></div><div class="section level2">
      <h2 id="neural-networks-functional-module">Neural networks functional module<a class="anchor" aria-label="anchor" href="#neural-networks-functional-module"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="nnf_adaptive_avg_pool1d.html">nnf_adaptive_avg_pool1d()</a></code> 
        </dt>
        <dd>Adaptive_avg_pool1d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_adaptive_avg_pool2d.html">nnf_adaptive_avg_pool2d()</a></code> 
        </dt>
        <dd>Adaptive_avg_pool2d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_adaptive_avg_pool3d.html">nnf_adaptive_avg_pool3d()</a></code> 
        </dt>
        <dd>Adaptive_avg_pool3d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_adaptive_max_pool1d.html">nnf_adaptive_max_pool1d()</a></code> 
        </dt>
        <dd>Adaptive_max_pool1d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_adaptive_max_pool2d.html">nnf_adaptive_max_pool2d()</a></code> 
        </dt>
        <dd>Adaptive_max_pool2d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_adaptive_max_pool3d.html">nnf_adaptive_max_pool3d()</a></code> 
        </dt>
        <dd>Adaptive_max_pool3d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_affine_grid.html">nnf_affine_grid()</a></code> 
        </dt>
        <dd>Affine_grid</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_alpha_dropout.html">nnf_alpha_dropout()</a></code> 
        </dt>
        <dd>Alpha_dropout</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_avg_pool1d.html">nnf_avg_pool1d()</a></code> 
        </dt>
        <dd>Avg_pool1d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_avg_pool2d.html">nnf_avg_pool2d()</a></code> 
        </dt>
        <dd>Avg_pool2d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_avg_pool3d.html">nnf_avg_pool3d()</a></code> 
        </dt>
        <dd>Avg_pool3d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_batch_norm.html">nnf_batch_norm()</a></code> 
        </dt>
        <dd>Batch_norm</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_bilinear.html">nnf_bilinear()</a></code> 
        </dt>
        <dd>Bilinear</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_binary_cross_entropy.html">nnf_binary_cross_entropy()</a></code> 
        </dt>
        <dd>Binary_cross_entropy</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_binary_cross_entropy_with_logits.html">nnf_binary_cross_entropy_with_logits()</a></code> 
        </dt>
        <dd>Binary_cross_entropy_with_logits</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_celu.html">nnf_celu()</a></code> <code><a href="nnf_celu.html">nnf_celu_()</a></code> 
        </dt>
        <dd>Celu</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_contrib_sparsemax.html">nnf_contrib_sparsemax()</a></code> 
        </dt>
        <dd>Sparsemax</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_conv1d.html">nnf_conv1d()</a></code> 
        </dt>
        <dd>Conv1d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_conv2d.html">nnf_conv2d()</a></code> 
        </dt>
        <dd>Conv2d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_conv3d.html">nnf_conv3d()</a></code> 
        </dt>
        <dd>Conv3d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_conv_tbc.html">nnf_conv_tbc()</a></code> 
        </dt>
        <dd>Conv_tbc</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_conv_transpose1d.html">nnf_conv_transpose1d()</a></code> 
        </dt>
        <dd>Conv_transpose1d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_conv_transpose2d.html">nnf_conv_transpose2d()</a></code> 
        </dt>
        <dd>Conv_transpose2d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_conv_transpose3d.html">nnf_conv_transpose3d()</a></code> 
        </dt>
        <dd>Conv_transpose3d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_cosine_embedding_loss.html">nnf_cosine_embedding_loss()</a></code> 
        </dt>
        <dd>Cosine_embedding_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_cosine_similarity.html">nnf_cosine_similarity()</a></code> 
        </dt>
        <dd>Cosine_similarity</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_cross_entropy.html">nnf_cross_entropy()</a></code> 
        </dt>
        <dd>Cross_entropy</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_ctc_loss.html">nnf_ctc_loss()</a></code> 
        </dt>
        <dd>Ctc_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_dropout.html">nnf_dropout()</a></code> 
        </dt>
        <dd>Dropout</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_dropout2d.html">nnf_dropout2d()</a></code> 
        </dt>
        <dd>Dropout2d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_dropout3d.html">nnf_dropout3d()</a></code> 
        </dt>
        <dd>Dropout3d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_elu.html">nnf_elu()</a></code> <code><a href="nnf_elu.html">nnf_elu_()</a></code> 
        </dt>
        <dd>Elu</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_embedding.html">nnf_embedding()</a></code> 
        </dt>
        <dd>Embedding</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_embedding_bag.html">nnf_embedding_bag()</a></code> 
        </dt>
        <dd>Embedding_bag</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_fold.html">nnf_fold()</a></code> 
        </dt>
        <dd>Fold</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_fractional_max_pool2d.html">nnf_fractional_max_pool2d()</a></code> 
        </dt>
        <dd>Fractional_max_pool2d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_fractional_max_pool3d.html">nnf_fractional_max_pool3d()</a></code> 
        </dt>
        <dd>Fractional_max_pool3d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_gelu.html">nnf_gelu()</a></code> 
        </dt>
        <dd>Gelu</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_glu.html">nnf_glu()</a></code> 
        </dt>
        <dd>Glu</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_grid_sample.html">nnf_grid_sample()</a></code> 
        </dt>
        <dd>Grid_sample</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_group_norm.html">nnf_group_norm()</a></code> 
        </dt>
        <dd>Group_norm</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_gumbel_softmax.html">nnf_gumbel_softmax()</a></code> 
        </dt>
        <dd>Gumbel_softmax</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_hardshrink.html">nnf_hardshrink()</a></code> 
        </dt>
        <dd>Hardshrink</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_hardsigmoid.html">nnf_hardsigmoid()</a></code> 
        </dt>
        <dd>Hardsigmoid</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_hardswish.html">nnf_hardswish()</a></code> 
        </dt>
        <dd>Hardswish</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_hardtanh.html">nnf_hardtanh()</a></code> <code><a href="nnf_hardtanh.html">nnf_hardtanh_()</a></code> 
        </dt>
        <dd>Hardtanh</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_hinge_embedding_loss.html">nnf_hinge_embedding_loss()</a></code> 
        </dt>
        <dd>Hinge_embedding_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_instance_norm.html">nnf_instance_norm()</a></code> 
        </dt>
        <dd>Instance_norm</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_interpolate.html">nnf_interpolate()</a></code> 
        </dt>
        <dd>Interpolate</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_kl_div.html">nnf_kl_div()</a></code> 
        </dt>
        <dd>Kl_div</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_l1_loss.html">nnf_l1_loss()</a></code> 
        </dt>
        <dd>L1_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_layer_norm.html">nnf_layer_norm()</a></code> 
        </dt>
        <dd>Layer_norm</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_leaky_relu.html">nnf_leaky_relu()</a></code> 
        </dt>
        <dd>Leaky_relu</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_linear.html">nnf_linear()</a></code> 
        </dt>
        <dd>Linear</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_local_response_norm.html">nnf_local_response_norm()</a></code> 
        </dt>
        <dd>Local_response_norm</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_log_softmax.html">nnf_log_softmax()</a></code> 
        </dt>
        <dd>Log_softmax</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_logsigmoid.html">nnf_logsigmoid()</a></code> 
        </dt>
        <dd>Logsigmoid</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_lp_pool1d.html">nnf_lp_pool1d()</a></code> 
        </dt>
        <dd>Lp_pool1d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_lp_pool2d.html">nnf_lp_pool2d()</a></code> 
        </dt>
        <dd>Lp_pool2d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_margin_ranking_loss.html">nnf_margin_ranking_loss()</a></code> 
        </dt>
        <dd>Margin_ranking_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_max_pool1d.html">nnf_max_pool1d()</a></code> 
        </dt>
        <dd>Max_pool1d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_max_pool2d.html">nnf_max_pool2d()</a></code> 
        </dt>
        <dd>Max_pool2d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_max_pool3d.html">nnf_max_pool3d()</a></code> 
        </dt>
        <dd>Max_pool3d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_max_unpool1d.html">nnf_max_unpool1d()</a></code> 
        </dt>
        <dd>Max_unpool1d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_max_unpool2d.html">nnf_max_unpool2d()</a></code> 
        </dt>
        <dd>Max_unpool2d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_max_unpool3d.html">nnf_max_unpool3d()</a></code> 
        </dt>
        <dd>Max_unpool3d</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_mse_loss.html">nnf_mse_loss()</a></code> 
        </dt>
        <dd>Mse_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_multi_head_attention_forward.html">nnf_multi_head_attention_forward()</a></code> 
        </dt>
        <dd>Multi head attention forward</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_multi_margin_loss.html">nnf_multi_margin_loss()</a></code> 
        </dt>
        <dd>Multi_margin_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_multilabel_margin_loss.html">nnf_multilabel_margin_loss()</a></code> 
        </dt>
        <dd>Multilabel_margin_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_multilabel_soft_margin_loss.html">nnf_multilabel_soft_margin_loss()</a></code> 
        </dt>
        <dd>Multilabel_soft_margin_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_nll_loss.html">nnf_nll_loss()</a></code> 
        </dt>
        <dd>Nll_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_normalize.html">nnf_normalize()</a></code> 
        </dt>
        <dd>Normalize</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_one_hot.html">nnf_one_hot()</a></code> 
        </dt>
        <dd>One_hot</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_pad.html">nnf_pad()</a></code> 
        </dt>
        <dd>Pad</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_pairwise_distance.html">nnf_pairwise_distance()</a></code> 
        </dt>
        <dd>Pairwise_distance</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_pdist.html">nnf_pdist()</a></code> 
        </dt>
        <dd>Pdist</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_pixel_shuffle.html">nnf_pixel_shuffle()</a></code> 
        </dt>
        <dd>Pixel_shuffle</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_poisson_nll_loss.html">nnf_poisson_nll_loss()</a></code> 
        </dt>
        <dd>Poisson_nll_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_prelu.html">nnf_prelu()</a></code> 
        </dt>
        <dd>Prelu</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_relu.html">nnf_relu()</a></code> <code><a href="nnf_relu.html">nnf_relu_()</a></code> 
        </dt>
        <dd>Relu</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_relu6.html">nnf_relu6()</a></code> 
        </dt>
        <dd>Relu6</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_rrelu.html">nnf_rrelu()</a></code> <code><a href="nnf_rrelu.html">nnf_rrelu_()</a></code> 
        </dt>
        <dd>Rrelu</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_selu.html">nnf_selu()</a></code> <code><a href="nnf_selu.html">nnf_selu_()</a></code> 
        </dt>
        <dd>Selu</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_sigmoid.html">nnf_sigmoid()</a></code> 
        </dt>
        <dd>Sigmoid</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_silu.html">nnf_silu()</a></code> 
        </dt>
        <dd>Applies the Sigmoid Linear Unit (SiLU) function, element-wise. See <code><a href="../reference/nn_silu.html">nn_silu()</a></code> for more information.</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_smooth_l1_loss.html">nnf_smooth_l1_loss()</a></code> 
        </dt>
        <dd>Smooth_l1_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_soft_margin_loss.html">nnf_soft_margin_loss()</a></code> 
        </dt>
        <dd>Soft_margin_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_softmax.html">nnf_softmax()</a></code> 
        </dt>
        <dd>Softmax</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_softmin.html">nnf_softmin()</a></code> 
        </dt>
        <dd>Softmin</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_softplus.html">nnf_softplus()</a></code> 
        </dt>
        <dd>Softplus</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_softshrink.html">nnf_softshrink()</a></code> 
        </dt>
        <dd>Softshrink</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_softsign.html">nnf_softsign()</a></code> 
        </dt>
        <dd>Softsign</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_tanhshrink.html">nnf_tanhshrink()</a></code> 
        </dt>
        <dd>Tanhshrink</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_threshold.html">nnf_threshold()</a></code> <code><a href="nnf_threshold.html">nnf_threshold_()</a></code> 
        </dt>
        <dd>Threshold</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_triplet_margin_loss.html">nnf_triplet_margin_loss()</a></code> 
        </dt>
        <dd>Triplet_margin_loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_triplet_margin_with_distance_loss.html">nnf_triplet_margin_with_distance_loss()</a></code> 
        </dt>
        <dd>Triplet margin with distance loss</dd>
      </dl><dl><dt>
          
          <code><a href="nnf_unfold.html">nnf_unfold()</a></code> 
        </dt>
        <dd>Unfold</dd>
      </dl></div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="torch_device.html">torch_device()</a></code> 
        </dt>
        <dd>Create a Device object</dd>
      </dl><dl><dt>
          
          <code><a href="local_device.html">local_device()</a></code> <code><a href="local_device.html">with_device()</a></code> 
        </dt>
        <dd>Device contexts</dd>
      </dl></div><div class="section level2">
      <h2 id="optimizers">Optimizers<a class="anchor" aria-label="anchor" href="#optimizers"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="optimizer.html">optimizer()</a></code> 
        </dt>
        <dd>Creates a custom optimizer</dd>
      </dl><dl><dt>
          
          <code><a href="optim_adadelta.html">optim_adadelta()</a></code> 
        </dt>
        <dd>Adadelta optimizer</dd>
      </dl><dl><dt>
          
          <code><a href="optim_adagrad.html">optim_adagrad()</a></code> 
        </dt>
        <dd>Adagrad optimizer</dd>
      </dl><dl><dt>
          
          <code><a href="optim_adam.html">optim_adam()</a></code> 
        </dt>
        <dd>Implements Adam algorithm.</dd>
      </dl><dl><dt>
          
          <code><a href="optim_adamw.html">optim_adamw()</a></code> 
        </dt>
        <dd>Implements AdamW algorithm</dd>
      </dl><dl><dt>
          
          <code><a href="optim_asgd.html">optim_asgd()</a></code> 
        </dt>
        <dd>Averaged Stochastic Gradient Descent optimizer</dd>
      </dl><dl><dt>
          
          <code><a href="optim_lbfgs.html">optim_lbfgs()</a></code> 
        </dt>
        <dd>LBFGS optimizer</dd>
      </dl><dl><dt>
          
          <code><a href="optim_required.html">optim_required()</a></code> 
        </dt>
        <dd>Dummy value indicating a required value.</dd>
      </dl><dl><dt>
          
          <code><a href="optim_rmsprop.html">optim_rmsprop()</a></code> 
        </dt>
        <dd>RMSprop optimizer</dd>
      </dl><dl><dt>
          
          <code><a href="optim_rprop.html">optim_rprop()</a></code> 
        </dt>
        <dd>Implements the resilient backpropagation algorithm.</dd>
      </dl><dl><dt>
          
          <code><a href="optim_sgd.html">optim_sgd()</a></code> 
        </dt>
        <dd>SGD optimizer</dd>
      </dl><dl><dt>
          
          <code><a href="is_optimizer.html">is_optimizer()</a></code> 
        </dt>
        <dd>Checks if the object is a torch optimizer</dd>
      </dl></div><div class="section level2">
      <h2 id="learning-rate-schedulers">Learning rate schedulers<a class="anchor" aria-label="anchor" href="#learning-rate-schedulers"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="lr_cosine_annealing.html">lr_cosine_annealing()</a></code> 
        </dt>
        <dd>Set the learning rate of each parameter group using a cosine annealing schedule</dd>
      </dl><dl><dt>
          
          <code><a href="lr_lambda.html">lr_lambda()</a></code> 
        </dt>
        <dd>Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.</dd>
      </dl><dl><dt>
          
          <code><a href="lr_multiplicative.html">lr_multiplicative()</a></code> 
        </dt>
        <dd>Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.</dd>
      </dl><dl><dt>
          
          <code><a href="lr_one_cycle.html">lr_one_cycle()</a></code> 
        </dt>
        <dd>Once cycle learning rate</dd>
      </dl><dl><dt>
          
          <code><a href="lr_reduce_on_plateau.html">lr_reduce_on_plateau()</a></code> 
        </dt>
        <dd>Reduce learning rate on plateau</dd>
      </dl><dl><dt>
          
          <code><a href="lr_scheduler.html">lr_scheduler()</a></code> 
        </dt>
        <dd>Creates learning rate schedulers</dd>
      </dl><dl><dt>
          
          <code><a href="lr_step.html">lr_step()</a></code> 
        </dt>
        <dd>Step learning rate decay</dd>
      </dl></div><div class="section level2">
      <h2 id="datasets">Datasets<a class="anchor" aria-label="anchor" href="#datasets"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="dataset.html">dataset()</a></code> 
        </dt>
        <dd>Helper function to create an function that generates R6 instances of class <code>dataset</code></dd>
      </dl><dl><dt>
          
          <code><a href="dataset_subset.html">dataset_subset()</a></code> 
        </dt>
        <dd>Dataset Subset</dd>
      </dl><dl><dt>
          
          <code><a href="iterable_dataset.html">iterable_dataset()</a></code> 
        </dt>
        <dd>Creates an iterable dataset</dd>
      </dl><dl><dt>
          
          <code><a href="dataloader.html">dataloader()</a></code> 
        </dt>
        <dd>Data loader. Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.</dd>
      </dl><dl><dt>
          
          <code><a href="dataloader_make_iter.html">dataloader_make_iter()</a></code> 
        </dt>
        <dd>Creates an iterator from a DataLoader</dd>
      </dl><dl><dt>
          
          <code><a href="dataloader_next.html">dataloader_next()</a></code> 
        </dt>
        <dd>Get the next element of a dataloader iterator</dd>
      </dl><dl><dt>
          
          <code><a href="enumerate.html">enumerate()</a></code> 
        </dt>
        <dd>Enumerate an iterator</dd>
      </dl><dl><dt>
          
          <code><a href="enumerate.dataloader.html">enumerate(<i>&lt;dataloader&gt;</i>)</a></code> 
        </dt>
        <dd>Enumerate an iterator</dd>
      </dl><dl><dt>
          
          <code><a href="tensor_dataset.html">tensor_dataset()</a></code> 
        </dt>
        <dd>Dataset wrapping tensors.</dd>
      </dl><dl><dt>
          
          <code><a href="is_dataloader.html">is_dataloader()</a></code> 
        </dt>
        <dd>Checks if the object is a dataloader</dd>
      </dl><dl><dt>
          
          <code><a href="sampler.html">sampler()</a></code> 
        </dt>
        <dd>Creates a new Sampler</dd>
      </dl></div><div class="section level2">
      <h2 id="distributions">Distributions<a class="anchor" aria-label="anchor" href="#distributions"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="Distribution.html">Distribution</a></code> 
        </dt>
        <dd>Generic R6 class representing distributions</dd>
      </dl><dl><dt>
          
          <code><a href="distr_bernoulli.html">distr_bernoulli()</a></code> 
        </dt>
        <dd>Creates a Bernoulli distribution parameterized by <code>probs</code> or <code>logits</code> (but not both). Samples are binary (0 or 1). They take the value <code>1</code> with probability <code>p</code> and <code>0</code> with probability <code>1 - p</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="distr_categorical.html">distr_categorical()</a></code> 
        </dt>
        <dd>Creates a categorical distribution parameterized by either <code>probs</code> or <code>logits</code> (but not both).</dd>
      </dl><dl><dt>
          
          <code><a href="distr_chi2.html">distr_chi2()</a></code> 
        </dt>
        <dd>Creates a Chi2 distribution parameterized by shape parameter <code>df</code>. This is exactly equivalent to <code>distr_gamma(alpha=0.5*df, beta=0.5)</code></dd>
      </dl><dl><dt>
          
          <code><a href="distr_gamma.html">distr_gamma()</a></code> 
        </dt>
        <dd>Creates a Gamma distribution parameterized by shape <code>concentration</code> and <code>rate</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="distr_mixture_same_family.html">distr_mixture_same_family()</a></code> 
        </dt>
        <dd>Mixture of components in the same family</dd>
      </dl><dl><dt>
          
          <code><a href="distr_multivariate_normal.html">distr_multivariate_normal()</a></code> 
        </dt>
        <dd>Gaussian distribution</dd>
      </dl><dl><dt>
          
          <code><a href="distr_normal.html">distr_normal()</a></code> 
        </dt>
        <dd>Creates a normal (also called Gaussian) distribution parameterized by <code>loc</code> and <code>scale</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="distr_poisson.html">distr_poisson()</a></code> 
        </dt>
        <dd>Creates a Poisson distribution parameterized by <code>rate</code>, the rate parameter.</dd>
      </dl><dl><dt>
          
          <code><a href="Constraint.html">Constraint</a></code> 
        </dt>
        <dd>Abstract base class for constraints.</dd>
      </dl></div><div class="section level2">
      <h2 id="autograd">Autograd<a class="anchor" aria-label="anchor" href="#autograd"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="autograd_backward.html">autograd_backward()</a></code> 
        </dt>
        <dd>Computes the sum of gradients of given tensors w.r.t. graph leaves.</dd>
      </dl><dl><dt>
          
          <code><a href="autograd_function.html">autograd_function()</a></code> 
        </dt>
        <dd>Records operation history and defines formulas for differentiating ops.</dd>
      </dl><dl><dt>
          
          <code><a href="autograd_grad.html">autograd_grad()</a></code> 
        </dt>
        <dd>Computes and returns the sum of gradients of outputs w.r.t. the inputs.</dd>
      </dl><dl><dt>
          
          <code><a href="autograd_set_grad_mode.html">autograd_set_grad_mode()</a></code> 
        </dt>
        <dd>Set grad mode</dd>
      </dl><dl><dt>
          
          <code><a href="with_no_grad.html">with_no_grad()</a></code> <code><a href="with_no_grad.html">local_no_grad()</a></code> 
        </dt>
        <dd>Temporarily modify gradient recording.</dd>
      </dl><dl><dt>
          
          <code><a href="with_enable_grad.html">with_enable_grad()</a></code> <code><a href="with_enable_grad.html">local_enable_grad()</a></code> 
        </dt>
        <dd>Enable grad</dd>
      </dl><dl><dt>
          
          <code><a href="with_detect_anomaly.html">with_detect_anomaly()</a></code> 
        </dt>
        <dd>Context-manager that enable anomaly detection for the autograd engine.</dd>
      </dl><dl><dt>
          
          <code><a href="AutogradContext.html">AutogradContext</a></code> 
        </dt>
        <dd>Class representing the context.</dd>
      </dl></div><div class="section level2">
      <h2 id="mixed-precision">Mixed precision<a class="anchor" aria-label="anchor" href="#mixed-precision"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="local_autocast.html">local_autocast()</a></code> <code><a href="local_autocast.html">with_autocast()</a></code> <code><a href="local_autocast.html">set_autocast()</a></code> <code><a href="local_autocast.html">unset_autocast()</a></code> 
        </dt>
        <dd>Autocast context manager</dd>
      </dl><dl><dt>
          
          <code><a href="cuda_amp_grad_scaler.html">cuda_amp_grad_scaler()</a></code> 
        </dt>
        <dd>Creates a gradient scaler</dd>
      </dl></div><div class="section level2">
      <h2 id="random-numbers">Random numbers<a class="anchor" aria-label="anchor" href="#random-numbers"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="torch_manual_seed.html">torch_manual_seed()</a></code> <code><a href="torch_manual_seed.html">local_torch_manual_seed()</a></code> <code><a href="torch_manual_seed.html">with_torch_manual_seed()</a></code> 
        </dt>
        <dd>Sets the seed for generating random numbers.</dd>
      </dl><dl><dt>
          
          <code><a href="torch_get_rng_state.html">torch_get_rng_state()</a></code> <code><a href="torch_get_rng_state.html">torch_set_rng_state()</a></code> <code><a href="torch_get_rng_state.html">cuda_get_rng_state()</a></code> <code><a href="torch_get_rng_state.html">cuda_set_rng_state()</a></code> 
        </dt>
        <dd>RNG state management</dd>
      </dl></div><div class="section level2">
      <h2 id="linear-algebra">Linear Algebra<a class="anchor" aria-label="anchor" href="#linear-algebra"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="linalg_cholesky.html">linalg_cholesky()</a></code> 
        </dt>
        <dd>Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_cholesky_ex.html">linalg_cholesky_ex()</a></code> 
        </dt>
        <dd>Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_cond.html">linalg_cond()</a></code> 
        </dt>
        <dd>Computes the condition number of a matrix with respect to a matrix norm.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_det.html">linalg_det()</a></code> 
        </dt>
        <dd>Computes the determinant of a square matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_eig.html">linalg_eig()</a></code> 
        </dt>
        <dd>Computes the eigenvalue decomposition of a square matrix if it exists.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_eigh.html">linalg_eigh()</a></code> 
        </dt>
        <dd>Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_eigvals.html">linalg_eigvals()</a></code> 
        </dt>
        <dd>Computes the eigenvalues of a square matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_eigvalsh.html">linalg_eigvalsh()</a></code> 
        </dt>
        <dd>Computes the eigenvalues of a complex Hermitian or real symmetric matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_householder_product.html">linalg_householder_product()</a></code> 
        </dt>
        <dd>Computes the first <code>n</code> columns of a product of Householder matrices.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_inv.html">linalg_inv()</a></code> 
        </dt>
        <dd>Computes the inverse of a square matrix if it exists.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_inv_ex.html">linalg_inv_ex()</a></code> 
        </dt>
        <dd>Computes the inverse of a square matrix if it is invertible.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_lstsq.html">linalg_lstsq()</a></code> 
        </dt>
        <dd>Computes a solution to the least squares problem of a system of linear equations.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_matrix_norm.html">linalg_matrix_norm()</a></code> 
        </dt>
        <dd>Computes a matrix norm.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_matrix_power.html">linalg_matrix_power()</a></code> 
        </dt>
        <dd>Computes the <code>n</code>-th power of a square matrix for an integer <code>n</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_matrix_rank.html">linalg_matrix_rank()</a></code> 
        </dt>
        <dd>Computes the numerical rank of a matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_multi_dot.html">linalg_multi_dot()</a></code> 
        </dt>
        <dd>Efficiently multiplies two or more matrices</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_norm.html">linalg_norm()</a></code> 
        </dt>
        <dd>Computes a vector or matrix norm.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_pinv.html">linalg_pinv()</a></code> 
        </dt>
        <dd>Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_qr.html">linalg_qr()</a></code> 
        </dt>
        <dd>Computes the QR decomposition of a matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_slogdet.html">linalg_slogdet()</a></code> 
        </dt>
        <dd>Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_solve.html">linalg_solve()</a></code> 
        </dt>
        <dd>Computes the solution of a square system of linear equations with a unique solution.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_solve_triangular.html">linalg_solve_triangular()</a></code> 
        </dt>
        <dd>Triangular solve</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_svd.html">linalg_svd()</a></code> 
        </dt>
        <dd>Computes the singular value decomposition (SVD) of a matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_svdvals.html">linalg_svdvals()</a></code> 
        </dt>
        <dd>Computes the singular values of a matrix.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_tensorinv.html">linalg_tensorinv()</a></code> 
        </dt>
        <dd>Computes the multiplicative inverse of <code><a href="../reference/torch_tensordot.html">torch_tensordot()</a></code></dd>
      </dl><dl><dt>
          
          <code><a href="linalg_tensorsolve.html">linalg_tensorsolve()</a></code> 
        </dt>
        <dd>Computes the solution <code>X</code> to the system <code>torch_tensordot(A, X) = B</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="linalg_vector_norm.html">linalg_vector_norm()</a></code> 
        </dt>
        <dd>Computes a vector norm.</dd>
      </dl></div><div class="section level2">
      <h2 id="cuda-utilities">Cuda utilities<a class="anchor" aria-label="anchor" href="#cuda-utilities"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="cuda_amp_grad_scaler.html">cuda_amp_grad_scaler()</a></code> 
        </dt>
        <dd>Creates a gradient scaler</dd>
      </dl><dl><dt>
          
          <code><a href="cuda_current_device.html">cuda_current_device()</a></code> 
        </dt>
        <dd>Returns the index of a currently selected device.</dd>
      </dl><dl><dt>
          
          <code><a href="cuda_device_count.html">cuda_device_count()</a></code> 
        </dt>
        <dd>Returns the number of GPUs available.</dd>
      </dl><dl><dt>
          
          <code><a href="cuda_empty_cache.html">cuda_empty_cache()</a></code> 
        </dt>
        <dd>Empty cache</dd>
      </dl><dl><dt>
          
          <code><a href="cuda_get_device_capability.html">cuda_get_device_capability()</a></code> 
        </dt>
        <dd>Returns the major and minor CUDA capability of <code>device</code></dd>
      </dl><dl><dt>
          
          <code><a href="cuda_is_available.html">cuda_is_available()</a></code> 
        </dt>
        <dd>Returns a bool indicating if CUDA is currently available.</dd>
      </dl><dl><dt>
          
          <code><a href="cuda_memory_stats.html">cuda_memory_stats()</a></code> <code><a href="cuda_memory_stats.html">cuda_memory_summary()</a></code> 
        </dt>
        <dd>Returns a dictionary of CUDA memory allocator statistics for a given device.</dd>
      </dl><dl><dt>
          
          <code><a href="cuda_runtime_version.html">cuda_runtime_version()</a></code> 
        </dt>
        <dd>Returns the CUDA runtime version</dd>
      </dl><dl><dt>
          
          <code><a href="cuda_synchronize.html">cuda_synchronize()</a></code> 
        </dt>
        <dd>Waits for all kernels in all streams on a CUDA device to complete.</dd>
      </dl><dl><dt>
          
          <code><a href="torch_get_rng_state.html">torch_get_rng_state()</a></code> <code><a href="torch_get_rng_state.html">torch_set_rng_state()</a></code> <code><a href="torch_get_rng_state.html">cuda_get_rng_state()</a></code> <code><a href="torch_get_rng_state.html">cuda_set_rng_state()</a></code> 
        </dt>
        <dd>RNG state management</dd>
      </dl></div><div class="section level2">
      <h2 id="jit">JIT<a class="anchor" aria-label="anchor" href="#jit"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="jit_compile.html">jit_compile()</a></code> 
        </dt>
        <dd>Compile TorchScript code into a graph</dd>
      </dl><dl><dt>
          
          <code><a href="jit_load.html">jit_load()</a></code> 
        </dt>
        <dd>Loads a <code>script_function</code> or <code>script_module</code> previously saved with <code>jit_save</code></dd>
      </dl><dl><dt>
          
          <code><a href="jit_ops.html">jit_ops</a></code> 
        </dt>
        <dd>Enable idiomatic access to JIT operators from R.</dd>
      </dl><dl><dt>
          
          <code><a href="jit_save.html">jit_save()</a></code> 
        </dt>
        <dd>Saves a <code>script_function</code> to a path</dd>
      </dl><dl><dt>
          
          <code><a href="jit_save_for_mobile.html">jit_save_for_mobile()</a></code> 
        </dt>
        <dd>Saves a <code>script_function</code> or <code>script_module</code> in bytecode form, to be loaded on a mobile device</dd>
      </dl><dl><dt>
          
          <code><a href="jit_scalar.html">jit_scalar()</a></code> 
        </dt>
        <dd>Adds the 'jit_scalar' class to the input</dd>
      </dl><dl><dt>
          
          <code><a href="jit_trace.html">jit_trace()</a></code> 
        </dt>
        <dd>Trace a function and return an executable <code>script_function</code>.</dd>
      </dl><dl><dt>
          
          <code><a href="jit_trace_module.html">jit_trace_module()</a></code> 
        </dt>
        <dd>Trace a module</dd>
      </dl><dl><dt>
          
          <code><a href="jit_tuple.html">jit_tuple()</a></code> 
        </dt>
        <dd>Adds the 'jit_tuple' class to the input</dd>
      </dl></div><div class="section level2">
      <h2 id="backends">Backends<a class="anchor" aria-label="anchor" href="#backends"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="backends_cudnn_is_available.html">backends_cudnn_is_available()</a></code> 
        </dt>
        <dd>CuDNN is available</dd>
      </dl><dl><dt>
          
          <code><a href="backends_cudnn_version.html">backends_cudnn_version()</a></code> 
        </dt>
        <dd>CuDNN version</dd>
      </dl><dl><dt>
          
          <code><a href="backends_mkl_is_available.html">backends_mkl_is_available()</a></code> 
        </dt>
        <dd>MKL is available</dd>
      </dl><dl><dt>
          
          <code><a href="backends_mkldnn_is_available.html">backends_mkldnn_is_available()</a></code> 
        </dt>
        <dd>MKLDNN is available</dd>
      </dl><dl><dt>
          
          <code><a href="backends_mps_is_available.html">backends_mps_is_available()</a></code> 
        </dt>
        <dd>MPS is available</dd>
      </dl><dl><dt>
          
          <code><a href="backends_openmp_is_available.html">backends_openmp_is_available()</a></code> 
        </dt>
        <dd>OpenMP is available</dd>
      </dl></div><div class="section level2">
      <h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="install_torch.html">install_torch()</a></code> 
        </dt>
        <dd>Install Torch</dd>
      </dl><dl><dt>
          
          <code><a href="install_torch_from_file.html">get_install_libs_url()</a></code> <code><a href="install_torch_from_file.html">install_torch_from_file()</a></code> 
        </dt>
        <dd>Install Torch from files</dd>
      </dl></div><div class="section level2">
      <h2 id="contrib">Contrib<a class="anchor" aria-label="anchor" href="#contrib"></a></h2>
      
      

      
    </div><div id="" class="section level2">
      
      
      

      <dl><dt>
          
          <code><a href="contrib_sort_vertices.html">contrib_sort_vertices()</a></code> 
        </dt>
        <dd>Contrib sort vertices</dd>
      </dl></div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Daniel Falbel, Javier Luraschi.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.</p>
</div>

    </footer></div>

  

  

  </body></html>

