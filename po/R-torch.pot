msgid ""
msgstr ""
"Project-Id-Version: torch 0.16.0\n"
"POT-Creation-Date: 2025-08-21 09:09+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: R7.R:46
msgid "can only set to `public`, `private` and `active`"
msgstr ""

#: autocast.R:63 autocast.R:87
msgid "Unsupported device {.val {device}}."
msgstr ""

#: autocast.R:153
msgid "{.var growth_factor} should be > 1 but got {.val {growth_factor}}."
msgstr ""

#: autocast.R:156
msgid "{.var backoff_factor} should be < 1 but got {.val {backoff_factor}}."
msgstr ""

#: autocast.R:175
msgid ""
"{.var outputs} device must be {.val cuda}, got {.val {outputs$device$type}}."
msgstr ""

#: autocast.R:188
msgid ""
"{.var outputs} must be a tensor or a list of tensors, got {.cls "
"{class(outputs)}}."
msgstr ""

#: autocast.R:197
msgid ""
"{.fn unscale_} has already been called on this optimizer since the last {.fn "
"update}."
msgstr ""

#: autocast.R:199
msgid "{.fn unscale_} is being called after {.fn step}."
msgstr ""

#: autocast.R:216
msgid "{.fn step} has already been called since the last {.fn update}."
msgstr ""

#: autocast.R:254
msgid "{.var .growth_tracker} initialized before {.var .scale}"
msgstr ""

#: autocast.R:263
msgid "Attempted {.fn {funcname}} but {.var .scale} is {.val NULL}."
msgstr ""

#: autocast.R:269
msgid "Attempted {.fn {funcname}} but {.var .growth_tracker} is {.val NULL}."
msgstr ""

#: autograd.R:34
msgid ""
"This mode should be enabled only for debugging as the different tests will "
"slow down your program execution."
msgstr ""

#: autograd.R:132
msgid "`keep_graph` has been deprecated. Please use `retain_graph` instead."
msgstr ""

#: autograd.R:133 install.R:179 install.R:192 install.R:194
msgid "i"
msgstr ""

#: autograd.R:133
msgid "`keep_graph` will take precedence."
msgstr ""

#: autograd.R:134
msgid "once"
msgstr ""

#: autograd.R:135
msgid "keep_graph"
msgstr ""

#: backends.R:16
msgid "CuDNN is not available."
msgstr ""

#: call_torch_function.R:39
msgid ""
"Because this function allows access to unexported functions, please use with "
"caution, and\n"
"            only if you are sure know what you are doing. Unexported "
"functions will expect inputs that\n"
"            are more C++-like than R-like. For example, they will expect all "
"indexes to be 0-based instead\n"
"            of 1-based. In addition unexported functions may be subject to "
"removal from the API without\n"
"            warning. Set quiet = TRUE to silence this warning."
msgstr ""

#: call_torch_function.R:47
msgid "Only functions prefixed with 'torch_' are available from this function."
msgstr ""

#: call_torch_function.R:54
msgid ""
"No function named {name} can be found. Please check your spelling and that "
"the desired function exists."
msgstr ""

#: codegen-utils.R:5
msgid "Dimension is 1-based, but found 0."
msgstr ""

#: codegen-utils.R:40
msgid "{fun_name} does not exist"
msgstr ""

#: conditions.R:2
msgid "value_error"
msgstr ""

#: conditions.R:6
msgid "type_error"
msgstr ""

#: conditions.R:10 utils-data-dataloader.R:499
msgid "runtime_error"
msgstr ""

#: conditions.R:14
msgid "not_implemented_error"
msgstr ""

#: conditions.R:18
msgid "warning"
msgstr ""

#: conditions.R:30
msgid "stop_iteration_error"
msgstr ""

#: conditions.R:36
msgid "deprecated"
msgstr ""

#: creation-ops.R:6 creation-ops.R:12
msgid "You should specify a single size argument."
msgstr ""

#: creation-ops.R:274
msgid "This function is deprecated in favor of torch_arange."
msgstr ""

#: creation-ops.R:394
msgid "values must be length 1"
msgstr ""

#: creation-ops.R:398
msgid "value must be a length 1 vector"
msgstr ""

#: cuda.R:42
msgid "device must be an integer between 0 and the number of devices minus 1"
msgstr ""

#: cuda.R:92
msgid "CUDA is not available."
msgstr ""

#: device.R:12
msgid ""
"type {type} should not include any index because index was passed explicitly."
msgstr ""

#: device.R:119
msgid "y is not a torch_device"
msgstr ""

#: distributions-bernoulli.R:21 distributions-categorical.R:18
msgid "Either `probs` or `logits` must be specified, but not both."
msgstr ""

#: distributions-categorical.R:23
msgid "`probs` must be at least one-dimensional."
msgstr ""

#: distributions-categorical.R:29
msgid "`logits` must be at least one-dimensional."
msgstr ""

#: distributions-constraints.R:38
msgid "Cannot determine validity of dependent constraint"
msgstr ""

#: distributions-constraints.R:60
msgid "Expected value$dim() >= {expected} but got {value$dim()}"
msgstr ""

#: distributions-mixture_same_family.R:20
msgid "Mixture distribution must be distr_categorical."
msgstr ""

#: distributions-mixture_same_family.R:24
msgid "Component distribution must be an instance of torch_Distribution."
msgstr ""

#: distributions-mixture_same_family.R:32
msgid ""
"Mixture distribution component ({km}) does not equal "
"component_distribution$batch_shape[-1] ({kc})."
msgstr ""

#: distributions-multivariate_normal.R:81
msgid "loc must be at least one-dimensional."
msgstr ""

#: distributions-multivariate_normal.R:85
msgid ""
"Exactly one of covariance_matrix or precision_matrix or scale_tril may be "
"specified."
msgstr ""

#: distributions-multivariate_normal.R:92
msgid "scale_tril matrix must be at least two-dimensional"
msgstr ""

#: distributions-multivariate_normal.R:93 distributions-multivariate_normal.R:103 distributions-multivariate_normal.R:115
msgid "with optional leading batch dimensions"
msgstr ""

#: distributions-multivariate_normal.R:102
msgid "covariance_matrix matrix must be at least two-dimensional"
msgstr ""

#: distributions-multivariate_normal.R:114
msgid "precision_matrix matrix must be at least two-dimensional"
msgstr ""

#: distributions-utils.R:24
msgid "Input arguments must all be instances of numeric,"
msgstr ""

#: distributions-utils.R:25
msgid "torch_tensor or objects implementing __torch_function__."
msgstr ""

#: distributions.R:169
msgid "The value argument to log_prob must be a Tensor"
msgstr ""

#: distributions.R:176
msgid ""
"The right-most size of value must match event_shape:\n"
"           {value$size()} vs {private$.event_shape}."
msgstr ""

#: distributions.R:192
msgid ""
"Value is not broadcastable with\n"
"             batch_shape+event_shape: {actual_shape} vs {expected_shape}."
msgstr ""

#: distributions.R:199
msgid "The value argument must be within the support"
msgstr ""

#: distributions.R:275
msgid "Subclass {paste0(class(self), collapse = ' ')} of"
msgstr ""

#: distributions.R:276
msgid "{paste0(class(cls), collapse = ' ')}"
msgstr ""

#: distributions.R:277
msgid "that defines a custom `initialize()` method"
msgstr ""

#: distributions.R:278
msgid "must also define a custom `expand()` method."
msgstr ""

#: dtype.R:165
msgid "One of the objects is not a dtype. Comparison is not possible."
msgstr ""

#: generator.R:17
msgid "bit64 is required to correctly show the seed."
msgstr ""

#: generator.R:24
msgid "Seed must be integer or integer64."
msgstr ""

#: generator.R:151
msgid ""
"Expected length {.var state} ({.val {length(state)}}) equal to the number of "
"cuda devices ({.val {cuda_device_count()}})."
msgstr ""

#: ignite.R:24
msgid "The `params` must have length > 0."
msgstr ""

#: ignite.R:62
msgid ""
"The `state_dict` must be a list with elements 'param_groups' and 'state'."
msgstr ""

#: ignite.R:67
msgid ""
"To-be loaded state dict is missing states for parameters "
"{paste(setdiff(names(prev_states), names(states)), collapse = ', ')}."
msgstr ""

#: ignite.R:71
msgid ""
"The {i}-th state has elements with names {paste0(names(prev_states[[i]]), "
"collapse = ', ')} but got {paste0(names(states[[i]]), collapse = ', ')}."
msgstr ""

#: ignite.R:108
msgid "The `params` must be a list of tensors."
msgstr ""

#: ignite.R:127
msgid ""
"Parameter groups must be a list of the same length as the number of "
"parameter groups."
msgstr ""

#: ignite.R:133
msgid ""
"Parameter groups must include names '{paste0(cpp_names, collapse = ', ')}' "
"but only included '{paste0(names(new_param_group), collapse = ', ')}'."
msgstr ""

#: ignite.R:143
msgid ""
"Cannot change the parameter groups, use `$add_param_group()` to add a new "
"parameter group."
msgstr ""

#: ignite.R:157 ignite.R:158 ignite.R:159 ignite.R:160 ignite.R:161 ignite.R:162 ignite.R:163
msgid "Abstract method"
msgstr ""

#: install.R:59
msgid "torch dependencies have been installed."
msgstr ""

#: install.R:60
msgid "You must restart your session to use {.pkg torch} correctly."
msgstr ""

#: install.R:86
msgid "We are now proceeding to download and installing lantern and torch."
msgstr ""

#: install.R:87
msgid "The installation path is: {.path {inst_path}}"
msgstr ""

#: install.R:92
msgid "An installation of {.strong {libname}} already exists."
msgstr ""

#: install.R:93
msgid "Found file at {.path {inst_path}}."
msgstr ""

#: install.R:149
msgid "Installation failed."
msgstr ""

#: install.R:150
msgid "Could not install {.strong {libname}} from {.val {url}}."
msgstr ""

#: install.R:178 install.R:191
msgid "x"
msgstr ""

#: install.R:178
msgid ""
"{.pkg torch} cannot write into configured {.var TORCH_HOME} {.path "
"{install_path}}."
msgstr ""

#: install.R:179
msgid "Please configure {.var TORCH_HOME} to be one of {.val {.libPath()}}"
msgstr ""

#: install.R:191
msgid "{.pkg torch} cannot write into {.path {install_path}}."
msgstr ""

#: install.R:192
msgid "Please configure a {.var TORCH_HOME} variable with a writable folder"
msgstr ""

#: install.R:193
msgid "and run {.fn install_torch()} again"
msgstr ""

#: install.R:194
msgid ""
"Or run R under the {.emph root} user {.strong once} to perform the {.fn "
"install_torch()}"
msgstr ""

#: install.R:195
msgid "if you use system level package manager like {.pkg r2u}"
msgstr ""

#: install.R:218
msgid "LibTorch will be downloaded from:"
msgstr ""

#: install.R:219
msgid "{.url {url}}"
msgstr ""

#: install.R:273
msgid "Could not find the SHA of the commit that installed the package."
msgstr ""

#: install.R:274
msgid "Using the latest build for the specified branch: {.val {branch}}."
msgstr ""

#: install.R:279
msgid "Could find the SHA of the commit that installed the package."
msgstr ""

#: install.R:280
msgid "SHA: {.val {remote_sha}}."
msgstr ""

#: install.R:293
msgid "Lantern will be downloaded from the following URL:"
msgstr ""

#: install.R:294
msgid "{.url {final_url}}"
msgstr ""

#: install.R:320
msgid "Architecture {.val {arch}} is not supported in this OS."
msgstr ""

#: install.R:324
msgid "Unsupported architecture {.val {arch}}."
msgstr ""

#: install.R:327
msgid "Architecture is {.val {arch}}"
msgstr ""

#: install.R:342
msgid "No cuda instalation has been found. Using {.val cpu}."
msgstr ""

#: install.R:345
msgid "{.envvar CUDA} is set to {.val cpu}, so using the {.val cpu}."
msgstr ""

#: install.R:349
msgid "Installation kind will be {.val {cu}}."
msgstr ""

#: install.R:362
msgid ""
"{.envvar CUDA} has been specified. The CUDA version is {.strong {version}}"
msgstr ""

#: install.R:374
msgid "Not on Windows or Linux. No CUDA installation supported."
msgstr ""

#: install.R:384
msgid "{.envvar CUDA_HOME}={.path {cuda_home}} is specified."
msgstr ""

#: install.R:386
msgid ""
"{.envvar CUDA_HOME} is not specified. Looking in conventional locations."
msgstr ""

#: install.R:427
msgid "{.envvar CUDA_PATH}={.path {cuda_path}}."
msgstr ""

#: install.R:428
msgid "Trying to find CUDA in this path."
msgstr ""

#: install.R:432
msgid "{.envvar CUDA_PATH} is not specified."
msgstr ""

#: install.R:433
msgid "Searching for installation in conventional locations."
msgstr ""

#: install.R:449
msgid "Trying to use the nvcc version that might be on your path."
msgstr ""

#: install.R:472
msgid "Unsupported CUDA version {.val {version}}"
msgstr ""

#: install.R:473
msgid "Currently supported versions are: {.val {supported_versions}}."
msgstr ""

#: install.R:496
msgid "Found CUDA version {.strong {cuda_version}}."
msgstr ""

#: install.R:497
msgid "This version was specified in {.path {versions_file}}"
msgstr ""

#: install.R:501
msgid "Could not find a CUDA version in {.path {versions_file}}."
msgstr ""

#: install.R:514
msgid ""
"Tried to query nvcc from {.path {nvcc_path}}, but was unable to find a CUDA "
"version."
msgstr ""

#: install.R:521
msgid "Found CUDA version {.strong {version}}."
msgstr ""

#: install.R:522
msgid "It was found by querying nvcc at {.path {nvcc_path}}."
msgstr ""

#: install.R:532
msgid "torch_install"
msgstr ""

#: install.R:541
msgid "Unexpected value"
msgstr ""

#: install.R:588
msgid ""
"Please use the env vars described in {.fn install_torch} to configure the "
"installation type."
msgstr ""

#: install.R:591
msgid "It's not possible to configure the libtorch version."
msgstr ""

#: install.R:600
msgid ""
"Follow the links to download the dependencies, then set the {.envvar "
"TORCH_URL} and {.envvar LANTERN_URL} env vars to the file paths."
msgstr ""

#: install.R:601
msgid "LibTorch: {.url {out$libtorch}}"
msgstr ""

#: install.R:602
msgid "LibLantern: {.url {out$liblantern}}"
msgstr ""

#: install.R:636
msgid ""
"This function is now deprecated. The same results can be achieved with {.fn "
"install_torch}."
msgstr ""

#: install.R:637
msgid ""
"Use the envvars {.envvar TORCH_URL} and {.envvar LANTERN_URL} to set the "
"file locations."
msgstr ""

#: install.R:657
msgid "Unable to download from {.url {url}}"
msgstr ""

#: install.R:658
msgid ""
"Please verify that the URL is not blocked by your firewall. See also {.url "
"https://torch.mlverse.org/docs/articles/installation.html#file-based-"
"download}"
msgstr ""

#: ivalue.R:12
msgid "Argument 'x' must be a list."
msgstr ""

#: ivalue.R:29
msgid "Argument 'x' must be scalar atomic."
msgstr ""

#: lantern_load.R:6
msgid "Torch is not installed, please run 'install_torch()'."
msgstr ""

#: linalg.R:269
msgid "`tol` argument is deprecated in favor of `atol` and `rtol`."
msgstr ""

#: linalg.R:954
msgid "`rcond` is deprecated in favor of `rtol`."
msgstr ""

#: nn-activation.R:760
msgid "embed_dim must be divisible by num_heads"
msgstr ""

#: nn-batchnorm.R:42 nn-init.R:229 nn-init.R:421
msgid "not implemented"
msgstr ""

#: nn-batchnorm.R:177
msgid "expected 2D or 3D input (got {input$dim()}D input)"
msgstr ""

#: nn-batchnorm.R:250
msgid "expected 4D input (got {input$dim()}D input)"
msgstr ""

#: nn-batchnorm.R:329
msgid "expected 5D input (got {input$dim()}D input)"
msgstr ""

#: nn-conv.R:10
msgid "`in_channels` must be divisible by groups"
msgstr ""

#: nn-conv.R:14
msgid "`out_channels` must be divisible by groups"
msgstr ""

#: nn-conv.R:21
msgid ""
"padding_mode must be one of [{paste(valid_padding_modes, collapse = ', ')}],"
msgstr ""

#: nn-conv.R:22
msgid "but got padding_mode='{padding_mode}'."
msgstr ""

#: nn-conv.R:509
msgid "Only `zeros` padding is supported."
msgstr ""

#: nn-conv.R:529
msgid "output_size must have {k} or {k+2} elements (got {length(output_size)})"
msgstr ""

#: nn-conv.R:550
msgid "requested an output of size {output_size}, but valid"
msgstr ""

#: nn-conv.R:551
msgid "sizes range from {min_size} to {max_size} (for an input"
msgstr ""

#: nn-conv.R:552
msgid "of size {input$size()[-c(1,2)]}"
msgstr ""

#: nn-conv.R:686
msgid "Only `zeros` padding mode is supported for ConvTranspose1d"
msgstr ""

#: nn-conv.R:843
msgid "Only `zeros` padding mode is supported for ConvTranspose2d"
msgstr ""

#: nn-conv.R:997
msgid "Only `zeros` padding mode is supported for ConvTranspose3d"
msgstr ""

#: nn-dropout.R:8
msgid "dropout probability has to be between 0 and 1 but got {p}"
msgstr ""

#: nn-init.R:18
msgid "mean is more than 2 std from [a, b] in nn.init.trunc_normal_."
msgstr ""

#: nn-init.R:19
msgid "The distribution of values may be incorrect."
msgstr ""

#: nn-init.R:89
msgid "Unsupported nonlinearity: {nonlinearity}"
msgstr ""

#: nn-loss.R:1001
msgid "only p == 1 or p == 2 are supported."
msgstr ""

#: nn-loss.R:1004
msgid "weight must be NULL or 1-dimensional"
msgstr ""

#: nn-pooling.R:685 nn-pooling.R:758
msgid "both output_size and output_ratio are NULL"
msgstr ""

#: nn-pooling.R:689 nn-pooling.R:762
msgid "both output_size and output_ratio are not NULL"
msgstr ""

#: nn-pooling.R:694 nn-pooling.R:767
msgid "output_ratio must be between 0 and 1."
msgstr ""

#: nn-rnn.R:37
msgid "dropout option adds dropout after all but last"
msgstr ""

#: nn-rnn.R:38
msgid "recurrent layer, so non-zero dropout expects"
msgstr ""

#: nn-rnn.R:39
msgid "num_layers greater than 1, but got dropout={dropout} and"
msgstr ""

#: nn-rnn.R:40
msgid "num_layers={num_layers}"
msgstr ""

#: nn-rnn.R:53
msgid "Unrecognized RNN mode: {mode}"
msgstr ""

#: nn-rnn.R:273
msgid "No cudnn backend for mode '{mode}'"
msgstr ""

#: nn-rnn.R:395
msgid "Unknown nonlinearity '{self$nonlinearity}'"
msgstr ""

#: nn-sparse.R:67 nn-sparse.R:70 nn-sparse.R:167 nn-sparse.R:170
msgid "`padding_idx` must be within num_embeddings"
msgstr ""

#: nn-sparse.R:87 nn-sparse.R:187
msgid "Shape of `.weight` does not match num_embeddings and embedding_dim"
msgstr ""

#: nn-transformer.R:80
msgid ""
"Unsupported activation string: {activation}. Use 'relu', 'gelu', or a "
"callable function."
msgstr ""

#: nn-transformer.R:89
msgid "activation must be a string ('relu' or 'gelu') or a function."
msgstr ""

#: nn-transformer.R:95
msgid ""
"Explicit src_mask should not be set when is_causal=TRUE. Use one or the "
"other."
msgstr ""

#: nn-transformer.R:178
msgid ""
"`encoder_layer` must be an `nn_module` (transformer encoder layer instance)."
msgstr ""

#: nn-utils.R:7
msgid "Input dimension should be at least {length(out_size) + 1}."
msgstr ""

#: nn.R:21
msgid "Forward method is not implemented"
msgstr ""

#: nn.R:32
msgid "Expected {.cls nn_module} but got object of type {.cls {class(module)}}"
msgstr ""

#: nn.R:36
msgid "Module {.str {name}} must be initialized"
msgstr ""

#: nn.R:113
msgid ""
"nn.Module.to only accepts floating point '\n"
"                      'dtypes, but got desired dtype {dtype}"
msgstr ""

#: nn.R:199
msgid "Could not find {key} in the state_dict."
msgstr ""

#: nn.R:286
msgid "Field `training` must be a logical flag."
msgstr ""

#: nn.R:295
msgid "It's not possible to modify the parameters list."
msgstr ""

#: nn.R:296 nn.R:312
msgid "You can modify the parameter in-place or use"
msgstr ""

#: nn.R:297 nn.R:313
msgid "`module$parameter_name <- new_value`"
msgstr ""

#: nn.R:311
msgid "It's not possible to modify the buffers list."
msgstr ""

#: nn.R:327
msgid "It's not possible to modify the modules list."
msgstr ""

#: nn.R:328 nn.R:348
msgid "You can modify the modules in-place"
msgstr ""

#: nn.R:347
msgid "It's not possible to modify the children list."
msgstr ""

#: nn.R:369
msgid "`x` must be a tensor."
msgstr ""

#: nn.R:846
msgid "All elements in {.arg dict} must be named."
msgstr ""

#: nn.R:852
msgid "{.fn nn_module_dict} has {.fn forward} implementation."
msgstr ""

#: nn_adaptive.R:108
msgid "not yet implemented"
msgstr ""

#: nnf-activation.R:81
msgid "not yet implemented."
msgstr ""

#: nnf-activation.R:802
msgid "Input should be a tensor and got '{class(input)}."
msgstr ""

#: nnf-dropout.R:42
msgid ""
"dropout2d: Received a {inp_dim}-D input to dropout2d, which is deprecated"
msgstr ""

#: nnf-dropout.R:43
msgid "and will result in an error in a future release. To retain the behavior"
msgstr ""

#: nnf-dropout.R:44
msgid ""
"and silence this warning, please use dropout instead. Note that dropout2d"
msgstr ""

#: nnf-dropout.R:45
msgid ""
"exists to provide channel-wise dropout on inputs with 2 spatial dimensions,"
msgstr ""

#: nnf-dropout.R:46
msgid ""
"a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs)."
msgstr ""

#: nnf-dropout.R:52
msgid ""
"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise"
msgstr ""

#: nnf-dropout.R:53
msgid ""
"1D dropout behavior is desired - input is interpreted as shape (N, C, L), "
"where C"
msgstr ""

#: nnf-dropout.R:54
msgid ""
"is the channel dim. This behavior will change in a future release to "
"interpret the"
msgstr ""

#: nnf-dropout.R:55
msgid ""
"input as one without a batch dimension, i.e. shape (C, H, W). To maintain "
"the 1D"
msgstr ""

#: nnf-dropout.R:56
msgid ""
"channel-wise dropout behavior, please switch to using dropout1d instead."
msgstr ""

#: nnf-embedding.R:83
msgid "if input is 2D, then offsets has to be NULL"
msgstr ""

#: nnf-loss.R:28
msgid ""
"reduction: 'mean' divides the total loss by both the batch size and the "
"support size."
msgstr ""

#: nnf-loss.R:29
msgid ""
"'batchmean' divides only by the batch size, and aligns with the KL div math "
"definition."
msgstr ""

#: nnf-loss.R:30
msgid ""
"'mean' will be changed to behave the same as 'batchmean' in the next major "
"release."
msgstr ""

#: nnf-loss.R:63
msgid ""
"Using a target size {target_shape} that is different to the input size "
"{input_shape}."
msgstr ""

#: nnf-loss.R:64
msgid "This will likely lead to incorrect results due to broadcasting."
msgstr ""

#: nnf-loss.R:65
msgid "Please ensure they have the same size."
msgstr ""

#: nnf-loss.R:216
msgid "reduction is not valid."
msgstr ""

#: nnf-pooling.R:278
msgid "output_size should be a sequence containing"
msgstr ""

#: nnf-pooling.R:279
msgid "{length(kernel_size)} or {length(kernel_size) + 2} elements"
msgstr ""

#: nnf-pooling.R:280
msgid "but it has a length of '{length(output_size)}'"
msgstr ""

#: nnf-pooling.R:452
msgid "output_ratio should not be NULL if output_size is NULL"
msgstr ""

#: nnf-upsampling.R:60
msgid "only one of size or scale_factor should be defined"
msgstr ""

#: nnf-upsampling.R:67
msgid ""
"size shape must match input shape. Input is {dim}D, size is {length(size)}"
msgstr ""

#: nnf-upsampling.R:77
msgid ""
"scale_factor shape must match input shape. Input is {dim}D, size is "
"{length(size)}"
msgstr ""

#: nnf-upsampling.R:82
msgid "either size or scale_factor should be defined"
msgstr ""

#: nnf-upsampling.R:144
msgid "Got 3D input, but bilinear mode needs 4D input"
msgstr ""

#: nnf-upsampling.R:148
msgid "Got 3D input, but trilinear mode needs 5D input"
msgstr ""

#: nnf-upsampling.R:152
msgid "Got 4D input, but trilinear mode needs 3D input"
msgstr ""

#: nnf-upsampling.R:164
msgid "Got 4D input, but trilinear mode needs 5D input"
msgstr ""

#: nnf-upsampling.R:168
msgid "Got 5D input, but trilinear mode needs 3D input"
msgstr ""

#: nnf-upsampling.R:172
msgid "Got 5D input, but bilinear mode needs 4D input"
msgstr ""

#: nnf-upsampling.R:192
msgid "Input Error: Only 3D, 4D and 5D input Tensors supported"
msgstr ""

#: nnf-upsampling.R:193
msgid ""
"(got {input$dim()}D) for the modes: nearest | linear | bilinear | bicubic | "
"trilinear"
msgstr ""

#: nnf-upsampling.R:194
msgid "(got {mode})"
msgstr ""

#: nnf-vision.R:97
msgid "Unknown mode name '{mode}'. Supported modes are 'bilinear'"
msgstr ""

#: nnf-vision.R:98
msgid "and 'nearest'."
msgstr ""

#: nnf-vision.R:111
msgid "Unknown padding mode name '{padding_mode}'. Supported modes are"
msgstr ""

#: nnf-vision.R:112
msgid "'zeros', 'border' and 'reflection'."
msgstr ""

#: operators.R:326 operators.R:334 operators.R:342 operators.R:358 operators.R:374
msgid "Torch tensors do not have NAs!"
msgstr ""

#: optim-adadelta.R:52 optim-adagrad.R:110 optim-adam.R:106 optim-adamw.R:90 optim-asgd.R:33 optim-rmsprop.R:105 optim-rprop.R:30 optim-sgd.R:106
msgid "Invalid learning rate: {lr}"
msgstr ""

#: optim-adadelta.R:56
msgid "Invalid rho value: {rho}"
msgstr ""

#: optim-adadelta.R:60 optim-adagrad.R:126 optim-adamw.R:93 optim-rmsprop.R:108
msgid "Invalid epsilon value: {eps}"
msgstr ""

#: optim-adadelta.R:64 optim-adagrad.R:118 optim-adamw.R:102 optim-asgd.R:37 optim-rmsprop.R:114 optim-sgd.R:114
msgid "Invalid weight_decay value: {weight_decay}"
msgstr ""

#: optim-adagrad.R:114
msgid "Invalid lr_decay value: {lr_decay}"
msgstr ""

#: optim-adagrad.R:122
msgid "Invalid initial_accumulator_value value: {initial_accumulator_value}"
msgstr ""

#: optim-adam.R:110
msgid "Invalid eps: {eps}"
msgstr ""

#: optim-adam.R:114
msgid "Invalid beta parameter at index 1"
msgstr ""

#: optim-adam.R:118
msgid "Invalid beta parameter at index 2"
msgstr ""

#: optim-adam.R:122
msgid "Invalid weight decay value: {weight_decay}"
msgstr ""

#: optim-adamw.R:96
msgid "Invalid betas[1] parameter value: {beta[1]}"
msgstr ""

#: optim-adamw.R:99
msgid "Invalid betas[2] parameter value: {beta[2]}"
msgstr ""

#: optim-lbfgs.R:372
msgid "LBFGS doesn't support per-parameter options"
msgstr ""

#: optim-lbfgs.R:373
msgid "(parameter groups)"
msgstr ""

#: optim-lbfgs.R:528
msgid "only strong_wolfe is supported"
msgstr ""

#: optim-lr_scheduler.R:18
msgid "not an optimizer"
msgstr ""

#: optim-lr_scheduler.R:35
msgid "param 'inital_lr' is not specified."
msgstr ""

#: optim-lr_scheduler.R:75
msgid "Adjusting learning rate of group %s to %.4f"
msgstr ""

#: optim-lr_scheduler.R:179 optim-lr_scheduler.R:237
msgid "lr_lambda length ({i}) is different from the number of"
msgstr ""

#: optim-lr_scheduler.R:180 optim-lr_scheduler.R:238
msgid "optimizer$param_grpups ({j})"
msgstr ""

#: optim-lr_scheduler.R:425
msgid "You must define either total_steps OR (epochs AND steps_per_epoch)"
msgstr ""

#: optim-lr_scheduler.R:428
msgid "Expected positive integer total_steps, but got {total_steps}"
msgstr ""

#: optim-lr_scheduler.R:434
msgid "Expected positive integer epochs, but got {epochs}"
msgstr ""

#: optim-lr_scheduler.R:438
msgid "Expected positive integer steps_per_epoch, but got {steps_per_epoch}"
msgstr ""

#: optim-lr_scheduler.R:449
msgid "Expected float between 0 and 1 pct_start, but got {pct_start}"
msgstr ""

#: optim-lr_scheduler.R:454
msgid ""
"anneal_strategy must by one of 'cos' or 'linear', instead got "
"{anneal_strategy}"
msgstr ""

#: optim-lr_scheduler.R:478
msgid "optimizer must support momentum with `cycle momentum` enabled"
msgstr ""

#: optim-lr_scheduler.R:506
msgid "expected {length(optimizer$param_groups)} values for {name}"
msgstr ""

#: optim-lr_scheduler.R:507
msgid "but got {length(param)}"
msgstr ""

#: optim-lr_scheduler.R:529
msgid ""
"Tried to step {step_num+1} times. The specified number of total steps is "
"{self$total_steps}"
msgstr ""

#: optim-lr_scheduler.R:632
msgid "Factor should be < 1.0"
msgstr ""

#: optim-lr_scheduler.R:639
msgid "expected {length(optimizer$param_groups} min_lrs, got {length(min_lr)}"
msgstr ""

#: optim-lr_scheduler.R:697
msgid "Epoch %s: reducing learning rate of group %s to %.4e"
msgstr ""

#: optim-lr_scheduler.R:721
msgid "mode {mode} is unknown!"
msgstr ""

#: optim-lr_scheduler.R:724
msgid "threshold mode {threshold_mode} is unknown!"
msgstr ""

#: optim-rmsprop.R:111 optim-sgd.R:110
msgid "Invalid momentum value: {momentum}"
msgstr ""

#: optim-rmsprop.R:117
msgid "Invalid alpha value: {alpha}"
msgstr ""

#: optim-rprop.R:33
msgid "Invalid eta values: {etas[[1]]}, {etas[[2]]}"
msgstr ""

#: optim-sgd.R:104
msgid "Need to specify a learning rate"
msgstr ""

#: optim-sgd.R:118
msgid "Nesterov momentum requires a momentum and zero dampening"
msgstr ""

#: optim.R:43
msgid "Wrong parameters specification."
msgstr ""

#: optim.R:54
msgid "`param_group` is not named"
msgstr ""

#: optim.R:65
msgid "optimizer can only optimize Tensors,"
msgstr ""

#: optim.R:66
msgid "but one of the params is {class(param)}"
msgstr ""

#: optim.R:71
msgid "can't optimize a non-leaf Tensor"
msgstr ""

#: optim.R:79
msgid "parameter group didn't specify a value of required"
msgstr ""

#: optim.R:80
msgid "optimization parameter {nm}"
msgstr ""

#: optim.R:119
msgid "Loaded state dict has a different number of parameter groups"
msgstr ""

#: optim.R:124
msgid ""
"Loaded state dict has contains a parameter group that doesn't match the size "
"of optimizers group."
msgstr ""

#: optim.R:165
msgid "OptimizerIgnite cannot be deep cloned"
msgstr ""

#: package.R:54
msgid "Failed to install torch, manually run {.fn install_torch}"
msgstr ""

#: package.R:84
msgid "torch failed to start, restart your R session to try again."
msgstr ""

#: package.R:85
msgid "You might need to reinstall torch using {.fn install_torch}"
msgstr ""

#: package.R:108
msgid "Aborted."
msgstr ""

#: package.R:117
msgid ""
"Currently only {.code x86_64} systems are supported for automatic "
"installation."
msgstr ""

#: package.R:118
msgid ""
"You can manually compile LibTorch for you architecture following "
"instructions in {.url https://github.com/pytorch/pytorch#from-source}"
msgstr ""

#: package.R:119
msgid "You can then use {.fn install_torch_from_file} to install manually."
msgstr ""

#: positron.R:20
msgid "Failed to register Positron methods."
msgstr ""

#: save.R:86
msgid "Cannot save `name` object."
msgstr ""

#: save.R:179
msgid ""
"Serializing objects with class {.cls {class(obj)}} is only supported with "
"serialization version >= 3, got {.val {use_ser_version()}}"
msgstr ""

#: save.R:256
msgid "Unexpected device {.val NULL}"
msgstr ""

#: save.R:282
msgid "File not saved with {.fn torch_save}, returning as is."
msgstr ""

#: save.R:283
msgid "Use {.fn safetensors::safe_load_file} to silence this warning."
msgstr ""

#: save.R:304
msgid "currently unsuported"
msgstr ""

#: save.R:319
msgid ""
"This version of torch can't load files with serialization version > "
"{ser_version}"
msgstr ""

#: save.R:488
msgid ""
"{.arg path} must be a connection or an actual path, got {.cls {class(path)}}."
msgstr ""

#: save.R:504
msgid ""
"{.arg path} must be a connection, a raw vector or an actual path, got {.cls "
"{class(path)}}."
msgstr ""

#: script_module.R:18
msgid "ScriptModule does not support non persistent buffers."
msgstr ""

#: script_module.R:29
msgid "Script modules can only register Script modules children."
msgstr ""

#: script_module.R:132
msgid ""
"Forward is not defined. Methods from submodules of traced modules are not "
"traced. Are you trying to call from a submodule?"
msgstr ""

#: tensor.R:58
msgid "Indexing starts at 1 and got a 0."
msgstr ""

#: tensor.R:82
msgid "Had {.arg other} but {.arg device} or {.arg dtype} are non {.val NULL}"
msgstr ""

#: tensor.R:122
msgid "You must pass a cuda device."
msgstr ""

#: tensor.R:193 tensor.R:200 wrapers.R:233
msgid "start indexing starts at 1"
msgstr ""

#: tensor.R:211 tensor.R:229
msgid "Can't set other and dim arguments."
msgstr ""

#: tensor.R:358
msgid "The tensor doesn't have names so you can't rename a dimension."
msgstr ""

#: tensor.R:451
msgid ""
"Converting integers > .Machine$integer.max is undefined and returns wrong "
"results. Use as.integer64(x)"
msgstr ""

#: trace.R:76
msgid "You must initialize the nn_module before tracing."
msgstr ""

#: trace.R:89
msgid "jit_trace needs a function or nn_module."
msgstr ""

#: trace.R:134 trace.R:432
msgid "Only `script_function` or `script_module` can be saved with `jit_save`."
msgstr ""

#: trace.R:155
msgid "Only `script_module` can be serialized with `jit_serialize`."
msgstr ""

#: trace.R:171
msgid "`obj` to be deserialized must be a raw vector."
msgstr ""

#: trace.R:327
msgid "`mod` must be a `nn_module()`."
msgstr ""

#: trace.R:331
msgid "`respect_mode` must be a logical(1)."
msgstr ""

#: trace.R:335
msgid "Arguments passed trough `...` must be named."
msgstr ""

#: trace.R:344
msgid "Methods `evalforward` and `trainforward` are reserved."
msgstr ""

#: trace.R:349
msgid "Method '{name}' does not exist in `mod` and therefore can't be traced."
msgstr ""

#: type-info.R:42
msgid "dtype must be an integer type."
msgstr ""

#: type-info.R:81
msgid "dtype must be a float type."
msgstr ""

#: utils-data-collate.R:31
msgid "Can't collate data of class: '{class(data)}'"
msgstr ""

#: utils-data-collate.R:46
msgid "Can't convert data of class: '{class(data)}'"
msgstr ""

#: utils-data-dataloader.R:121
msgid "Could not find an object with name '{names(worker_globals)[b]}'."
msgstr ""

#: utils-data-dataloader.R:158
msgid "Unknown dataset type with class {.cls {class(dataset)}}"
msgstr ""

#: utils-data-dataloader.R:211
msgid "Multi-process dataloader not implemented yet for Iterable datasets."
msgstr ""

#: utils-data-dataloader.R:405
msgid "Could not create a connection with the main process."
msgstr ""

#: utils-data-dataloader.R:441
msgid "Failed starting the worker."
msgstr ""

#: utils-data-dataloader.R:488 utils-data-dataloader.R:531
msgid "dataloader worker timed out."
msgstr ""

#: utils-data-dataloader.R:497
msgid "Error when getting dataset item."
msgstr ""

#: utils-data-dataloader.R:644
msgid ""
"Datasets used with parallel dataloader (num_workers > 0) shouldn't have "
"fields containing tensors as they can't be correctly passed to the wroker "
"subprocesses."
msgstr ""

#: utils-data-dataloader.R:645
msgid "A field named '{nm}' exists."
msgstr ""

#: utils-data-enum.R:41
msgid ""
"The `enumerate` construct is deprecated in favor of the `coro::loop` syntax."
msgstr ""

#: utils-data-enum.R:42
msgid "See https://github.com/mlverse/torch/issues/558 for more information."
msgstr ""

#: utils-data.R:33
msgid ""
"Loading the state_dict is only implemented when {.arg .refer_to_state_dict} "
"is {.val TRUE}"
msgstr ""

#: utils-data.R:226
msgid "all tensors must have the same size in the first dimension."
msgstr ""

#: wrapers.R:25
msgid "argument 'out' must be a list of Tensors."
msgstr ""

#: wrapers.R:30
msgid ""
"expected tuple of {2 + as.integer(get_infos)} elements but got {length(out)}"
msgstr ""

#: wrapers.R:105
msgid "argument 'window_length' must be int, not NULL"
msgstr ""

#: wrapers.R:183
msgid "tensordot expects dims >= 1, but got {dims}"
msgstr ""

#: wrapers.R:498
msgid "size is set, but one of mean or std is not a scalar value."
msgstr ""

#: wrapers.R:504
msgid "options is set, but one of mean or std is not a scalar value."
msgstr ""

#: wrapers.R:513
msgid "size is not set."
msgstr ""

#: wrapers.R:552
msgid "Please report a bug report in GitHub"
msgstr ""
