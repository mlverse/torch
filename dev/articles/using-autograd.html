<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Using autograd • torch</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Using autograd">
<meta name="robots" content="noindex">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-9ZJSKW3L0N"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9ZJSKW3L0N');
</script>
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">torch</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">0.14.1.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/installation.html">Installation</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Tensors</h6></li>
    <li><a class="dropdown-item" href="../articles/tensor-creation.html">Creating tensors</a></li>
    <li><a class="dropdown-item" href="../articles/indexing.html">Indexing</a></li>
    <li><a class="dropdown-item" href="../articles/tensor/index.html">Tensor class</a></li>
    <li><a class="dropdown-item" href="../articles/serialization.html">Serialization</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Datasets</h6></li>
    <li><a class="dropdown-item" href="../articles/loading-data.html">Loading Data</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Autograd</h6></li>
    <li><a class="dropdown-item" href="../articles/using-autograd.html">Using autograd</a></li>
    <li><a class="dropdown-item" href="../articles/extending-autograd.html">Extending autograd</a></li>
    <li><a class="dropdown-item" href="../articles/python-to-r.html">Python models</a></li>
    <li><a class="dropdown-item" href="../articles/torchscript.html">Jit Compilation</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-examples" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Examples</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-examples">
<li><a class="dropdown-item" href="../articles/examples/basic-autograd.html">basic-autograd</a></li>
    <li><a class="dropdown-item" href="../articles/examples/basic-nn-module.html">basic-nn-module</a></li>
    <li><a class="dropdown-item" href="../articles/examples/dataset.html">dataset</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-advanced" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Advanced</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-advanced">
<li><a class="dropdown-item" href="../articles/memory-management.html">Memory management</a></li>
    <li><a class="dropdown-item" href="../articles/modifying-source-code.html">Building locally</a></li>
    <li><a class="dropdown-item" href="../articles/amp.html">Automatic Mixed Precision</a></li>
    <li><a class="dropdown-item" href="../articles/modifying-source-code.html">Modifying source code</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/mlverse/torch/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Using autograd</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/mlverse/torch/blob/main/vignettes/using-autograd.Rmd" class="external-link"><code>vignettes/using-autograd.Rmd</code></a></small>
      <div class="d-none name"><code>using-autograd.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://torch.mlverse.org/docs">torch</a></span><span class="op">)</span></span></code></pre></div>
<p>So far, all we’ve been using from torch is <em>tensors</em>, but
we’ve been performing all calculations ourselves – the computing the
predictions, the loss, the gradients (and thus, the necessary updates to
the weights), and the new weight values. In this chapter, we’ll make a
significant change: Namely, we spare ourselves the cumbersome
calculation of gradients, and have torch do it for us.</p>
<p>Before we see that in action, let’s get some more background.</p>
<div class="section level2">
<h2 id="automatic-differentiation-with-autograd">Automatic differentiation with autograd<a class="anchor" aria-label="anchor" href="#automatic-differentiation-with-autograd"></a>
</h2>
<p>Torch uses a module called <em>autograd</em> to record operations
performed on tensors, and store what has to be done to obtain the
respective gradients. These actions are stored as functions, and those
functions are applied in order when the gradient of the output
(normally, the loss) with respect to those tensors is calculated:
starting from the output node and <em>propagating</em> gradients
<em>back</em> through the network. This is a form of <em>reverse mode
automatic differentiation</em>.</p>
<p>As users, we can see a bit of this implementation. As a prerequisite
for this “recording” to happen, tensors have to be created with
<code>requires_grad = TRUE</code>. E.g.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/torch_ones.html">torch_ones</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>To be clear, this is a tensor <em>with respect to which</em>
gradients have to be calculated – normally, a tensor representing a
weight or a bias, not the input data <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Unless we &lt;em&gt;want&lt;/em&gt; to change the data, as in
adversarial example generation&lt;/p&gt;"><sup>1</sup></a>. If we now perform some operation on that
tensor, assigning the result to <code>y</code></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>we find that <code>y</code> now has a non-empty <code>grad_fn</code>
that tells torch how to compute the gradient of <code>y</code> with
respect to <code>x</code>:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y</span><span class="op">$</span><span class="va">grad_fn</span></span>
<span><span class="co">#&gt; MeanBackward0</span></span></code></pre></div>
<p>Actual computation of gradients is triggered by calling
<code>backward()</code> on the output tensor.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>That executed, <code>x</code> now has a non-empty field
<code>grad</code> that stores the gradient of <code>y</code> with
respect to <code>x</code>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span><span class="op">$</span><span class="va">grad</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  0.2500  0.2500</span></span>
<span><span class="co">#&gt;  0.2500  0.2500</span></span>
<span><span class="co">#&gt; [ CPUFloatType{2,2} ]</span></span></code></pre></div>
<p>With a longer chain of computations, we can peek at how torch builds
up a graph of backward operations.</p>
<p>Here is a slightly more complex example. We call
<code>retain_grad()</code> on <code>y</code> and <code>z</code> just for
demonstration purposes; by default, intermediate gradients – while of
course they have to be computed – aren’t stored, in order to save
memory.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/torch_ones.html">torch_ones</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/torch_tensor.html">torch_tensor</a></span><span class="op">(</span><span class="fl">1.1</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x1</span> <span class="op">*</span> <span class="op">(</span><span class="va">x2</span> <span class="op">+</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">y</span><span class="op">$</span><span class="fu">retain_grad</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">z</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">$</span><span class="fu">pow</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fl">3</span></span>
<span><span class="va">z</span><span class="op">$</span><span class="fu">retain_grad</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">out</span> <span class="op">&lt;-</span> <span class="va">z</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Starting from <code>out$grad_fn</code>, we can follow the graph all
back to the leaf nodes:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># how to compute the gradient for mean, the last operation executed</span></span>
<span><span class="va">out</span><span class="op">$</span><span class="va">grad_fn</span></span>
<span><span class="co">#&gt; MeanBackward0</span></span>
<span><span class="co"># how to compute the gradient for the multiplication by 3 in z = y$pow(2) * 3</span></span>
<span><span class="va">out</span><span class="op">$</span><span class="va">grad_fn</span><span class="op">$</span><span class="va">next_functions</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; MulBackward1</span></span>
<span><span class="co"># how to compute the gradient for pow in z = y.pow(2) * 3</span></span>
<span><span class="va">out</span><span class="op">$</span><span class="va">grad_fn</span><span class="op">$</span><span class="va">next_functions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">next_functions</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; PowBackward0</span></span>
<span><span class="co"># how to compute the gradient for the multiplication in y = x * (x + 2)</span></span>
<span><span class="va">out</span><span class="op">$</span><span class="va">grad_fn</span><span class="op">$</span><span class="va">next_functions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">next_functions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">next_functions</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; MulBackward0</span></span>
<span><span class="co"># how to compute the gradient for the two branches of y = x * (x + 2),</span></span>
<span><span class="co"># where the left branch is a leaf node (AccumulateGrad for x1)</span></span>
<span><span class="va">out</span><span class="op">$</span><span class="va">grad_fn</span><span class="op">$</span><span class="va">next_functions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">next_functions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">next_functions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">next_functions</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; torch::autograd::AccumulateGrad</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]</span></span>
<span><span class="co">#&gt; AddBackward1</span></span>
<span><span class="co"># here we arrive at the other leaf node (AccumulateGrad for x2)</span></span>
<span><span class="va">out</span><span class="op">$</span><span class="va">grad_fn</span><span class="op">$</span><span class="va">next_functions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">next_functions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">next_functions</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">next_functions</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">next_functions</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; torch::autograd::AccumulateGrad</span></span></code></pre></div>
<p>After calling <code>out$backward()</code>, all tensors in the graph
will have their respective gradients created. Without our calls to
<code>retain_grad</code> above, <code>z$grad</code> and
<code>y$grad</code> would be empty:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">out</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">z</span><span class="op">$</span><span class="va">grad</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  0.2500  0.2500</span></span>
<span><span class="co">#&gt;  0.2500  0.2500</span></span>
<span><span class="co">#&gt; [ CPUFloatType{2,2} ]</span></span>
<span><span class="va">y</span><span class="op">$</span><span class="va">grad</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  4.6500  4.6500</span></span>
<span><span class="co">#&gt;  4.6500  4.6500</span></span>
<span><span class="co">#&gt; [ CPUFloatType{2,2} ]</span></span>
<span><span class="va">x2</span><span class="op">$</span><span class="va">grad</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  18.6000</span></span>
<span><span class="co">#&gt; [ CPUFloatType{1} ]</span></span>
<span><span class="va">x1</span><span class="op">$</span><span class="va">grad</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  14.4150  14.4150</span></span>
<span><span class="co">#&gt;  14.4150  14.4150</span></span>
<span><span class="co">#&gt; [ CPUFloatType{2,2} ]</span></span></code></pre></div>
<p>Thus acquainted with autograd, we’re ready to modify our example.</p>
</div>
<div class="section level2">
<h2 id="the-simple-network-now-using-autograd">The simple network, now using autograd<a class="anchor" aria-label="anchor" href="#the-simple-network-now-using-autograd"></a>
</h2>
<p>For a single new line calling <code>loss$backward()</code>, now a
number of lines (that did manual backprop) are gone:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">### generate training data -----------------------------------------------------</span></span>
<span><span class="co"># input dimensionality (number of input features)</span></span>
<span><span class="va">d_in</span> <span class="op">&lt;-</span> <span class="fl">3</span></span>
<span><span class="co"># output dimensionality (number of predicted features)</span></span>
<span><span class="va">d_out</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="co"># number of observations in training set</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="co"># create random data</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">n</span>, <span class="va">d_in</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span><span class="op">*</span><span class="fl">0.2</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="va">..</span>,<span class="fl">2</span><span class="op">]</span><span class="op">*</span><span class="fl">1.3</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="va">..</span>,<span class="fl">3</span><span class="op">]</span><span class="op">*</span><span class="fl">0.5</span> <span class="op">+</span> <span class="fu"><a href="../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">$</span><span class="fu">unsqueeze</span><span class="op">(</span>dim <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">### initialize weights ---------------------------------------------------------</span></span>
<span><span class="co"># dimensionality of hidden layer</span></span>
<span><span class="va">d_hidden</span> <span class="op">&lt;-</span> <span class="fl">32</span></span>
<span><span class="co"># weights connecting input to hidden layer</span></span>
<span><span class="va">w1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">d_in</span>, <span class="va">d_hidden</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co"># weights connecting hidden to output layer</span></span>
<span><span class="va">w2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">d_hidden</span>, <span class="va">d_out</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co"># hidden layer bias</span></span>
<span><span class="va">b1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/torch_zeros.html">torch_zeros</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">d_hidden</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co"># output layer bias</span></span>
<span><span class="va">b2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/torch_zeros.html">torch_zeros</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">d_out</span>,requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">### network parameters ---------------------------------------------------------</span></span>
<span><span class="va">learning_rate</span> <span class="op">&lt;-</span> <span class="fl">1e-4</span></span>
<span><span class="co">### training loop --------------------------------------------------------------</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">200</span><span class="op">)</span> <span class="op">{</span></span>
<span></span>
<span>    <span class="co">### -------- Forward pass -------- </span></span>
<span>    <span class="va">y_pred</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">$</span><span class="fu">mm</span><span class="op">(</span><span class="va">w1</span><span class="op">)</span><span class="op">$</span><span class="fu">add</span><span class="op">(</span><span class="va">b1</span><span class="op">)</span><span class="op">$</span><span class="fu">clamp</span><span class="op">(</span>min <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">$</span><span class="fu">mm</span><span class="op">(</span><span class="va">w2</span><span class="op">)</span><span class="op">$</span><span class="fu">add</span><span class="op">(</span><span class="va">b2</span><span class="op">)</span></span>
<span>    <span class="co">### -------- compute loss -------- </span></span>
<span>    <span class="va">loss</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">y_pred</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span><span class="op">$</span><span class="fu">pow</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span><span class="op">$</span><span class="fu">mean</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">t</span> <span class="op"><a href="https://rdrr.io/r/base/Arithmetic.html" class="external-link">%%</a></span> <span class="fl">10</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">t</span>, <span class="fu"><a href="../reference/as_array.html">as_array</a></span><span class="op">(</span><span class="va">loss</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>    <span class="co">### -------- Backpropagation -------- </span></span>
<span>    <span class="co"># compute the gradient of loss with respect to all tensors with requires_grad = True.</span></span>
<span>    <span class="va">loss</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span></span>
<span> </span>
<span>    <span class="co">### -------- Update weights -------- </span></span>
<span>    </span>
<span>    <span class="co"># Wrap in torch.no_grad() because this is a part we DON'T want to record for automatic gradient computation</span></span>
<span>    <span class="fu"><a href="../reference/with_no_grad.html">with_no_grad</a></span><span class="op">(</span><span class="op">{</span></span>
<span>      </span>
<span>      <span class="va">w1</span><span class="op">$</span><span class="fu">sub_</span><span class="op">(</span><span class="va">learning_rate</span> <span class="op">*</span> <span class="va">w1</span><span class="op">$</span><span class="va">grad</span><span class="op">)</span></span>
<span>      <span class="va">w2</span><span class="op">$</span><span class="fu">sub_</span><span class="op">(</span><span class="va">learning_rate</span> <span class="op">*</span> <span class="va">w2</span><span class="op">$</span><span class="va">grad</span><span class="op">)</span></span>
<span>      <span class="va">b1</span><span class="op">$</span><span class="fu">sub_</span><span class="op">(</span><span class="va">learning_rate</span> <span class="op">*</span> <span class="va">b1</span><span class="op">$</span><span class="va">grad</span><span class="op">)</span></span>
<span>      <span class="va">b2</span><span class="op">$</span><span class="fu">sub_</span><span class="op">(</span><span class="va">learning_rate</span> <span class="op">*</span> <span class="va">b2</span><span class="op">$</span><span class="va">grad</span><span class="op">)</span></span>
<span>      </span>
<span>      <span class="co"># Zero the gradients after every pass, because they'd accumulate otherwise</span></span>
<span>      <span class="va">w1</span><span class="op">$</span><span class="va">grad</span><span class="op">$</span><span class="fu">zero_</span><span class="op">(</span><span class="op">)</span></span>
<span>      <span class="va">w2</span><span class="op">$</span><span class="va">grad</span><span class="op">$</span><span class="fu">zero_</span><span class="op">(</span><span class="op">)</span></span>
<span>      <span class="va">b1</span><span class="op">$</span><span class="va">grad</span><span class="op">$</span><span class="fu">zero_</span><span class="op">(</span><span class="op">)</span></span>
<span>      <span class="va">b2</span><span class="op">$</span><span class="va">grad</span><span class="op">$</span><span class="fu">zero_</span><span class="op">(</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="op">}</span><span class="op">)</span></span>
<span>    </span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; 10 53.39279 </span></span>
<span><span class="co">#&gt; 20 48.18108 </span></span>
<span><span class="co">#&gt; 30 43.60909 </span></span>
<span><span class="co">#&gt; 40 39.58902 </span></span>
<span><span class="co">#&gt; 50 36.04592 </span></span>
<span><span class="co">#&gt; 60 32.91418 </span></span>
<span><span class="co">#&gt; 70 30.1415 </span></span>
<span><span class="co">#&gt; 80 27.68606 </span></span>
<span><span class="co">#&gt; 90 25.51679 </span></span>
<span><span class="co">#&gt; 100 23.58184 </span></span>
<span><span class="co">#&gt; 110 21.85429 </span></span>
<span><span class="co">#&gt; 120 20.30842 </span></span>
<span><span class="co">#&gt; 130 18.92111 </span></span>
<span><span class="co">#&gt; 140 17.67605 </span></span>
<span><span class="co">#&gt; 150 16.55709 </span></span>
<span><span class="co">#&gt; 160 15.54987 </span></span>
<span><span class="co">#&gt; 170 14.64194 </span></span>
<span><span class="co">#&gt; 180 13.82296 </span></span>
<span><span class="co">#&gt; 190 13.0857 </span></span>
<span><span class="co">#&gt; 200 12.41805</span></span></code></pre></div>
<p>We still manually compute the forward pass, and we still manually
update the weights. In the last two chapters of this section, we’ll see
how these parts of the logic can be made more modular and reusable, as
well.</p>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Daniel Falbel, Javier Luraschi.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
