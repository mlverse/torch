<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="torch">
<title>TorchScript • torch</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="TorchScript">
<meta property="og:description" content="torch">
<meta name="robots" content="noindex">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-9ZJSKW3L0N"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9ZJSKW3L0N');
</script>
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-dark navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">torch</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">0.11.0.9002</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/installation.html">Installation</a>
    <h6 class="dropdown-header" data-toc-skip>Tensors</h6>
    <a class="dropdown-item" href="../articles/tensor-creation.html">Creating tensors</a>
    <a class="dropdown-item" href="../articles/indexing.html">Indexing</a>
    <a class="dropdown-item" href="../articles/tensor/index.html">Tensor class</a>
    <a class="dropdown-item" href="../articles/serialization.html">Serialization</a>
    <h6 class="dropdown-header" data-toc-skip>Datasets</h6>
    <a class="dropdown-item" href="../articles/loading-data.html">Loading Data</a>
    <h6 class="dropdown-header" data-toc-skip>Autograd</h6>
    <a class="dropdown-item" href="../articles/using-autograd.html">Using autograd</a>
    <a class="dropdown-item" href="../articles/extending-autograd.html">Extending autograd</a>
    <a class="dropdown-item" href="../articles/python-to-r.html">Python models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-examples">Examples</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-examples">
    <a class="dropdown-item" href="../articles/examples/basic-autograd.html">basic-autograd</a>
    <a class="dropdown-item" href="../articles/examples/basic-nn-module.html">basic-nn-module</a>
    <a class="dropdown-item" href="../articles/examples/dataset.html">dataset</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-advanced">Advanced</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-advanced">
    <a class="dropdown-item" href="../articles/memory-management.html">Memory management</a>
    <a class="dropdown-item" href="../articles/modifying-source-code.html">Building locally</a>
    <a class="dropdown-item" href="../articles/amp.html">Automatic Mixed Precision</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/mlverse/torch/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>TorchScript</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/mlverse/torch/blob/HEAD/vignettes/torchscript.Rmd" class="external-link"><code>vignettes/torchscript.Rmd</code></a></small>
      <div class="d-none name"><code>torchscript.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://torch.mlverse.org/docs">torch</a></span><span class="op">)</span></span></code></pre></div>
<p><a href="https://pytorch.org/docs/stable/jit_language_reference.html#language-reference" class="external-link">TorchScript</a>
is a statically typed subset of Python that can be interpreted by
LibTorch without any Python dependency. The torch R package provides
interfaces to create, serialize, load and execute TorchScript
programs.</p>
<p>Advantages of using TorchScript are:</p>
<ul>
<li><p>TorchScript code can be invoked in its own interpreter, which is
basically a restricted Python interpreter. This interpreter does not
acquire the Global Interpreter Lock, and so many requests can be
processed on the same instance simultaneously.</p></li>
<li><p>This format allows us to save the whole model to disk and load it
into another environment, such as on server written in a language other
than R.</p></li>
<li><p>TorchScript gives us a representation in which we can do compiler
optimizations on the code to make execution more efficient.</p></li>
<li><p>TorchScript allows us to interface with many backend/device
runtimes that require a broader view of the program than individual
operators.</p></li>
</ul>
<div class="section level2">
<h2 id="creating-torchscript-programs">Creating TorchScript programs<a class="anchor" aria-label="anchor" href="#creating-torchscript-programs"></a>
</h2>
<div class="section level3">
<h3 id="tracing">Tracing<a class="anchor" aria-label="anchor" href="#tracing"></a>
</h3>
<p>TorchScript programs can be created from R using tracing. When using
tracing, code is automatically converted into this subset of Python by
recording only the actual operators on tensors and simply executing and
discarding the other surrounding R code.</p>
<p>Currently tracing is the only supported way to create TorchScript
programs from R code.</p>
<p>For example, let’s use the <code>jit_trace</code> function to create
a TorchScript program. We pass a regular R function and example
inputs.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="../reference/torch_relu.html">torch_relu</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">traced_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit_trace.html">jit_trace</a></span><span class="op">(</span><span class="va">fn</span>, <span class="fu"><a href="../reference/torch_tensor.html">torch_tensor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The <code>jit_trace</code> function has executed the R function with
the example input and recorded all torch operations that occurred during
execution to create a <em>graph</em>. <em>graph</em> is how we call the
intermediate representation of TorchScript programs, and it can be
inspected with:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">traced_fn</span><span class="op">$</span><span class="va">graph</span></span>
<span><span class="co">#&gt; graph(%0 : Float(3, strides=[1], requires_grad=0, device=cpu)):</span></span>
<span><span class="co">#&gt;   %1 : Float(3, strides=[1], requires_grad=0, device=cpu) = aten::relu(%0)</span></span>
<span><span class="co">#&gt;   return (%1)</span></span></code></pre></div>
<p>The traced function can now be invoked as a regular R function:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">traced_fn</span><span class="op">(</span><span class="fu"><a href="../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  0.0000</span></span>
<span><span class="co">#&gt;  0.0000</span></span>
<span><span class="co">#&gt;  0.1016</span></span>
<span><span class="co">#&gt; [ CPUFloatType{3} ]</span></span></code></pre></div>
<p>It’s also possible to trace <code>nn_modules()</code> defined in R,
for example:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">module</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nn_module.html">nn_module</a></span><span class="op">(</span></span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">linear1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nn_linear.html">nn_linear</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">10</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">linear2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nn_linear.html">nn_linear</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  forward <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="va">self</span><span class="op">$</span><span class="fu">linear1</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu"><a href="../reference/nnf_relu.html">nnf_relu</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="va">self</span><span class="op">$</span><span class="fu">linear2</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span><span class="va">traced_module</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit_trace.html">jit_trace</a></span><span class="op">(</span><span class="fu">module</span><span class="op">(</span><span class="op">)</span>, <span class="fu"><a href="../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>When using <code>jit_trace</code> with a <code>nn_module</code> only
the <code>forward</code> method is traced. You can use the
<code>jit_trace_module</code> function to pass example inputs to other
methods. Traced modules look like normal <code>nn_modules()</code>, and
can be called the same way:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">traced_module</span><span class="op">(</span><span class="fu"><a href="../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt; -0.7553</span></span>
<span><span class="co">#&gt; -0.3895</span></span>
<span><span class="co">#&gt; -0.5399</span></span>
<span><span class="co">#&gt; [ CPUFloatType{3,1} ][ grad_fn = &lt;AddmmBackward0&gt; ]</span></span></code></pre></div>
<div class="section level4">
<h4 id="limitations-of-tracing">Limitations of tracing<a class="anchor" aria-label="anchor" href="#limitations-of-tracing"></a>
</h4>
<ol style="list-style-type: decimal">
<li>Tracing will not record any control flow like if-statements or
loops. When this control flow is constant across your module, this is
fine and it often inlines the control flow decisions. But sometimes the
control flow is actually part of the model itself. For instance, a
recurrent network is a loop over the (possibly dynamic) length of an
input sequence. For example:</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># fn does does an operation for each dimension of a tensor</span></span>
<span><span class="va">fn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">x</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu"><a href="../reference/torch_unbind.html">torch_unbind</a></span><span class="op">(</span>dim <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">$</span><span class="fu">sum</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> </span>
<span>    <span class="fu"><a href="../reference/torch_stack.html">torch_stack</a></span><span class="op">(</span>dim <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co"># we trace using as an example a tensor with size (10, 5, 5)</span></span>
<span><span class="va">traced_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit_trace.html">jit_trace</a></span><span class="op">(</span><span class="va">fn</span>, <span class="fu"><a href="../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">5</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># applying it with a tensor with different size returns an error.</span></span>
<span><span class="fu">traced_fn</span><span class="op">(</span><span class="fu"><a href="../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="fl">11</span>, <span class="fl">5</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Error in cpp_call_traced_fn(ptr, inputs): The following operation failed in the TorchScript interpreter.</span></span>
<span><span class="co">#&gt; Traceback of TorchScript (most recent call last):</span></span>
<span><span class="co">#&gt; RuntimeError: Expected 10 elements in a list but found 11</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>In the returned <code>ScriptModule</code>, operations that have
different behaviors in training and eval modes will always behave as if
it were in the mode it was in during tracing, no matter which mode the
<code>ScriptModule</code> is in. For example:</li>
</ol>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">traced_dropout</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit_trace.html">jit_trace</a></span><span class="op">(</span><span class="fu"><a href="../reference/nn_dropout.html">nn_dropout</a></span><span class="op">(</span><span class="op">)</span>, <span class="fu"><a href="../reference/torch_ones.html">torch_ones</a></span><span class="op">(</span><span class="fl">5</span>,<span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">traced_dropout</span><span class="op">(</span><span class="fu"><a href="../reference/torch_ones.html">torch_ones</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  0  0  2</span></span>
<span><span class="co">#&gt;  2  2  2</span></span>
<span><span class="co">#&gt;  0  2  0</span></span>
<span><span class="co">#&gt; [ CPUFloatType{3,3} ]</span></span>
<span><span class="va">traced_dropout</span><span class="op">$</span><span class="fu">eval</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co"># even after setting to eval mode, dropout is applied</span></span>
<span><span class="fu">traced_dropout</span><span class="op">(</span><span class="fu"><a href="../reference/torch_ones.html">torch_ones</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  2  0  0</span></span>
<span><span class="co">#&gt;  0  0  0</span></span>
<span><span class="co">#&gt;  0  0  0</span></span>
<span><span class="co">#&gt; [ CPUFloatType{3,3} ]</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Tracing proegrams can only take tensors and lists of tensors as
input and return tensors and lists of tensors. For example:</li>
</ol>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">x</span> <span class="op">+</span> <span class="va">y</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="../reference/jit_trace.html">jit_trace</a></span><span class="op">(</span><span class="va">fn</span>, <span class="fu"><a href="../reference/torch_tensor.html">torch_tensor</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; Error in cpp_trace_function(tr_fn, list(...), .compilation_unit, strict, : Only tensors or (possibly nested) dict or tuples of tensors can be inputs to traced functions. Got float</span></span>
<span><span class="co">#&gt; Exception raised from addInput at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/frontend/tracer.cpp:422 (most recent call first):</span></span>
<span><span class="co">#&gt; frame #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;) + 81 (0x10b18eca1 in libc10.dylib)</span></span>
<span><span class="co">#&gt; frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;) + 98 (0x10b18d342 in libc10.dylib)</span></span>
<span><span class="co">#&gt; frame #2: torch::jit::tracer::addInput(std::__1::shared_ptr&lt;torch::jit::tracer::TracingState&gt; const&amp;, c10::IValue const&amp;, c10::Type::SingletonOrSharedTypePtr&lt;c10::Type&gt; const&amp;, torch::jit::Value*) + 6052 (0x1322ae564 in libtorch_cpu.dylib)</span></span>
<span><span class="co">#&gt; frame #3: torch::jit::tracer::addInput(std::__1::shared_ptr&lt;torch::jit::tracer::TracingState&gt; const&amp;, c10::IValue const&amp;, c10::Type::SingletonOrSharedTypePtr&lt;c10::Type&gt; const&amp;, torch::jit::Value*) + 4445 (0x1322adf1d in libtorch_cpu.dylib)</span></span>
<span><span class="co">#&gt; frame #4: torch::jit::tracer::trace(std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;, std::__1::function&lt;std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt; (std::__1::vector&lt;c10::IValue, std::__1::allocator&lt;c10::IValue&gt; &gt;)&gt; const&amp;, std::__1::function&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; (at::Tensor const&amp;)&gt;, bool, bool, torch::jit::Module*, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) + 826 (0x1322ab59a in libtorch_cpu.dylib)</span></span>
<span><span class="co">#&gt; frame #5: _lantern_trace_fn + 637 (0x11303600d in liblantern.dylib)</span></span>
<span><span class="co">#&gt; frame #6: cpp_trace_function(Rcpp::Function_Impl&lt;Rcpp::PreserveStorage&gt;, XPtrTorchStack, XPtrTorchCompilationUnit, XPtrTorchstring, bool, XPtrTorchScriptModule, bool, bool) + 547 (0x110eaf7a3 in torchpkg.so)</span></span>
<span><span class="co">#&gt; frame #7: _torch_cpp_trace_function + 727 (0x110cc0217 in torchpkg.so)</span></span>
<span><span class="co">#&gt; frame #8: R_doDotCall + 13245 (0x1084f387d in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #9: bcEval + 110576 (0x108543ab0 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #10: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #11: R_execClosure + 2289 (0x108549e31 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #12: Rf_applyClosure + 483 (0x108548ae3 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #13: bcEval + 28279 (0x10852f937 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #14: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #15: R_execClosure + 2289 (0x108549e31 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #16: Rf_applyClosure + 483 (0x108548ae3 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #17: Rf_eval + 1252 (0x108528724 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #18: do_eval + 634 (0x10854e0ba in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #19: bcEval + 30501 (0x1085301e5 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #20: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #21: R_execClosure + 2289 (0x108549e31 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #22: Rf_applyClosure + 483 (0x108548ae3 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #23: bcEval + 28279 (0x10852f937 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #24: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #25: R_execClosure + 2289 (0x108549e31 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #26: Rf_applyClosure + 483 (0x108548ae3 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #27: bcEval + 28279 (0x10852f937 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #28: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #29: forcePromise + 172 (0x1085481ac in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #30: Rf_eval + 701 (0x1085284fd in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #31: do_withVisible + 57 (0x10854e739 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #32: do_internal + 362 (0x10859c6ea in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #33: bcEval + 30966 (0x1085303b6 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #34: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #35: R_execClosure + 2289 (0x108549e31 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #36: Rf_applyClosure + 483 (0x108548ae3 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #37: bcEval + 28279 (0x10852f937 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #38: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #39: forcePromise + 172 (0x1085481ac in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #40: getvar + 762 (0x1085559fa in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #41: bcEval + 15430 (0x10852c706 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #42: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #43: R_execClosure + 2289 (0x108549e31 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #44: Rf_applyClosure + 483 (0x108548ae3 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #45: bcEval + 28279 (0x10852f937 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #46: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #47: forcePromise + 172 (0x1085481ac in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #48: getvar + 762 (0x1085559fa in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #49: bcEval + 15430 (0x10852c706 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #50: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #51: forcePromise + 172 (0x1085481ac in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #52: getvar + 762 (0x1085559fa in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #53: bcEval + 15430 (0x10852c706 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #54: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #55: forcePromise + 172 (0x1085481ac in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #56: getvar + 762 (0x1085559fa in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #57: bcEval + 15430 (0x10852c706 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #58: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #59: forcePromise + 172 (0x1085481ac in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #60: getvar + 762 (0x1085559fa in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #61: bcEval + 15430 (0x10852c706 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #62: Rf_eval + 532 (0x108528454 in libR.dylib)</span></span>
<span><span class="co">#&gt; frame #63: forcePromise + 172 (0x1085481ac in libR.dylib)</span></span>
<span><span class="co">#&gt; :</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="compiling-torchscript">Compiling TorchScript<a class="anchor" aria-label="anchor" href="#compiling-torchscript"></a>
</h3>
<p>It’s also possible to create TorchScript programs by compiling
TorchScript code. TorchScript code looks a lot like standard python
code. For example:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit_compile.html">jit_compile</a></span><span class="op">(</span><span class="st">"</span></span>
<span><span class="st">def fn (x: Tensor):</span></span>
<span><span class="st">  return torch.relu(x)</span></span>
<span><span class="st"></span></span>
<span><span class="st">"</span><span class="op">)</span></span>
<span><span class="va">tr</span><span class="op">$</span><span class="fu">fn</span><span class="op">(</span><span class="fu"><a href="../reference/torch_tensor.html">torch_tensor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  0</span></span>
<span><span class="co">#&gt;  0</span></span>
<span><span class="co">#&gt;  1</span></span>
<span><span class="co">#&gt; [ CPUFloatType{3} ]</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="serializing-and-loading">Serializing and loading<a class="anchor" aria-label="anchor" href="#serializing-and-loading"></a>
</h2>
<p>TorchScript programs can be serialized using the
<code>jit_save</code> function and loaded back from disk with
<code>jit_load</code>.</p>
<p>For example:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="../reference/torch_relu.html">torch_relu</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">tr_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit_trace.html">jit_trace</a></span><span class="op">(</span><span class="va">fn</span>, <span class="fu"><a href="../reference/torch_tensor.html">torch_tensor</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/jit_save.html">jit_save</a></span><span class="op">(</span><span class="va">tr_fn</span>, <span class="st">"path.pt"</span><span class="op">)</span></span>
<span><span class="va">loaded</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/jit_load.html">jit_load</a></span><span class="op">(</span><span class="st">"path.pt"</span><span class="op">)</span></span></code></pre></div>
<p>Loaded programs can be executed as usual:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">loaded</span><span class="op">(</span><span class="fu"><a href="../reference/torch_tensor.html">torch_tensor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  0</span></span>
<span><span class="co">#&gt;  0</span></span>
<span><span class="co">#&gt;  1</span></span>
<span><span class="co">#&gt; [ CPUFloatType{3} ]</span></span></code></pre></div>
<p><strong>Note</strong> You can load TorchScript programs that were
created in libraries different than <code>torch</code> for R. Eg, a
TorchScript program can be created in PyTorch with
<code>torch.jit.trace</code> or <code>torch.jit.script</code>, and run
from R.</p>
<p>R objects are automatically converted to their TorchScript
counterpart following the Types table in this document. However,
sometimes it’s necessary to make type annotations with
<code><a href="../reference/jit_tuple.html">jit_tuple()</a></code> and <code><a href="../reference/jit_scalar.html">jit_scalar()</a></code> to disambiguate
the conversion.</p>
</div>
<div class="section level2">
<h2 id="types">Types<a class="anchor" aria-label="anchor" href="#types"></a>
</h2>
<p>The following table lists all TorchScript types and how to convert
the to and back to R.</p>
<table class="table">
<colgroup>
<col width="24%">
<col width="75%">
</colgroup>
<thead><tr class="header">
<th>TorchScript Type</th>
<th>R Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>Tensor</code></td>
<td>A <code>torch_tensor</code> with any shape, dtype or backend.</td>
</tr>
<tr class="even">
<td><code>Tuple[T0, T1, ..., TN]</code></td>
<td>A <code><a href="https://rdrr.io/r/base/list.html" class="external-link">list()</a></code> containing subtypes <code>T0</code>,
<code>T1</code>, etc. wrapped with <code><a href="../reference/jit_tuple.html">jit_tuple()</a></code> .</td>
</tr>
<tr class="odd">
<td><code>bool</code></td>
<td>A scalar logical value create using <code>jit_scalar</code>.</td>
</tr>
<tr class="even">
<td><code>int</code></td>
<td>A scalar integer value created using <code>jit_scalar</code>.</td>
</tr>
<tr class="odd">
<td><code>float</code></td>
<td>A scalar floating value created using <code>jit_scalar</code>.</td>
</tr>
<tr class="even">
<td><code>str</code></td>
<td>A string (ie. character vector of length 1) wrapped in
<code>jit_scalar</code>.</td>
</tr>
<tr class="odd">
<td><code>List[T]</code></td>
<td>An R list of which all types are type <code>T</code> . Or numeric
vectors, logical vectors, etc.</td>
</tr>
<tr class="even">
<td><code>Optional[T]</code></td>
<td>Not yet supported.</td>
</tr>
<tr class="odd">
<td><code>Dict[str, V]</code></td>
<td>A named list with values of type <code>V</code> . Only
<code>str</code> key values are currently supported.</td>
</tr>
<tr class="even">
<td><code>T</code></td>
<td>Not yet supported.</td>
</tr>
<tr class="odd">
<td><code>E</code></td>
<td>Not yet supported.</td>
</tr>
<tr class="even">
<td><code>NamedTuple[T0, T1, ...]</code></td>
<td>A named list containing subtypes <code>T0</code>, <code>T1</code>,
etc. wrapped in <code><a href="../reference/jit_tuple.html">jit_tuple()</a></code>.</td>
</tr>
</tbody>
</table>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Daniel Falbel, Javier Luraschi.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
