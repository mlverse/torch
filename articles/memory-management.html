<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Memory management • torch</title>
<!-- mathjax math --><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script><script>
  window.MathJax = {
    chtml: {
      fontURL: "https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/output/chtml/fonts/woff-v2"
    }
  };
</script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Memory management">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-9ZJSKW3L0N"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9ZJSKW3L0N');
</script>
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">torch</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.16.3</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/installation.html">Installation</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Tensors</h6></li>
    <li><a class="dropdown-item" href="../articles/tensor-creation.html">Creating tensors</a></li>
    <li><a class="dropdown-item" href="../articles/indexing.html">Indexing</a></li>
    <li><a class="dropdown-item" href="../articles/tensor/index.html">Tensor class</a></li>
    <li><a class="dropdown-item" href="../articles/serialization.html">Serialization</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Datasets</h6></li>
    <li><a class="dropdown-item" href="../articles/loading-data.html">Loading Data</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Autograd</h6></li>
    <li><a class="dropdown-item" href="../articles/using-autograd.html">Using autograd</a></li>
    <li><a class="dropdown-item" href="../articles/extending-autograd.html">Extending autograd</a></li>
    <li><a class="dropdown-item" href="../articles/python-to-r.html">Python models</a></li>
    <li><a class="dropdown-item" href="../articles/torchscript.html">Jit Compilation</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-examples" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Examples</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-examples">
<li><a class="dropdown-item" href="../articles/examples/basic-autograd.html">basic-autograd</a></li>
    <li><a class="dropdown-item" href="../articles/examples/basic-nn-module.html">basic-nn-module</a></li>
    <li><a class="dropdown-item" href="../articles/examples/dataset.html">dataset</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-advanced" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Advanced</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-advanced">
<li><a class="dropdown-item" href="../articles/memory-management.html">Memory management</a></li>
    <li><a class="dropdown-item" href="../articles/modifying-source-code.html">Building locally</a></li>
    <li><a class="dropdown-item" href="../articles/amp.html">Automatic Mixed Precision</a></li>
    <li><a class="dropdown-item" href="../articles/modifying-source-code.html">Modifying source code</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/mlverse/torch/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Memory management</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/mlverse/torch/blob/main/vignettes/articles/memory-management.Rmd" class="external-link"><code>vignettes/articles/memory-management.Rmd</code></a></small>
      <div class="d-none name"><code>memory-management.Rmd</code></div>
    </div>

    
    
<p>Differently from most R objects, tensors created in torch have their
memory managed by LibTorch allocators. This means that functions like
<code><a href="https://rdrr.io/r/utils/object.size.html" class="external-link">object.size()</a></code> or <code>lobstr::mem_used()</code> do
<strong>not</strong> correctly report memory used.</p>
<p>The R garbage collector is very lazy, and is only called when R needs
more memory from the OS. Since R is not aware of large chunks of memory
that might be in use by torch tensors, it might not call the garbage
collector as often as it would if it knew that tensors are using more
memory. It’s common then that, even tensors that are no longer in use by
the R session are still alive in the R session (and thus using memory)
because they still din’t get garbage collected.</p>
<p>To solve this problem, the torch package has implemented strategies
to automatically call the R garbage collector when LibTorch is
allocating more memory. The strategies are different depending on where
the memory is being allocated: on the CPU, GPU (CUDA devices) or <a href="https://developer.apple.com/documentation/metalperformanceshaders" class="external-link">MPS</a>
(on Apple Silicon machines/ or equipped with AMD GPU’s).</p>
<div class="section level2">
<h2 id="cpu">CPU<a class="anchor" aria-label="anchor" href="#cpu"></a>
</h2>
<p>On the CPU, torch will possibly call the R garbage collector in two
moments:</p>
<ol style="list-style-type: decimal">
<li>
<p>Every 4GB of memory allocated by LibTorch we make a call to the R
garbage collector so it cleans up dangling tensors. The 4GB threshold
can be controled by setting the option
<code>torch.threshold_call_gc</code>, for example using:</p>
<pre><code><span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>torch.threshold_call_gc <span class="op">=</span> <span class="fl">4000</span><span class="op">)</span></span></code></pre>
<p>This option must be set before calling <code><a href="https://torch.mlverse.org/docs">library(torch)</a></code> or
calling any torch function for the first time, as this setting is
applied when torch starts up.</p>
</li>
<li>
<p>If torch fails allocating enough memory for for creating a new
tensor, the garbage collector is called and the allocation is retried.
Note: in some OS’s (specially the UNIX based) it’s very hard for an
allocation to fail if it’s not too large, because the system tries to
use swap. If too much swaping is used it’s possible that the system
hangs completely.</p>
<p>If your R session is hanging, and you are convinced that it should
have enough memory for the operations, try setting a lower value for the
<code>torch.threshold_call_gc</code> option, with this you will call the
GC more often and make sure tensors are quickly released from memory.
Note though, that calling the GC too often adds a lot of overhead, so
this will probably slow down the program execution.</p>
</li>
</ol>
</div>
<div class="section level2">
<h2 id="cuda">CUDA<a class="anchor" aria-label="anchor" href="#cuda"></a>
</h2>
<p>CUDA memory tends to be scarcer than CPU memory, also, allocation
must be faster otherwise allocation overhead can counterbalance the
speed up of GPU. To make allocations very fast and to avoid
segmentation, LibTorch uses a caching allocator to manage the GPU
memory, ie. once LibTorch allocated CUDA memory it won’t give it back to
the operation system, instead it reuses that memory for future
allocations. This means that <code>nvidia-smi</code> or
<code>nvtop</code> will not report the amount of memory used by tensors,
but the memory LibTorch has reserved from the OS. You can use
<code><a href="../reference/cuda_memory_stats.html">torch::cuda_memory_summary()</a></code> to query exactly the memory
used by LibTorch.</p>
<p>Like the CPU allocator, torch’s CUDA allocator will also call the R
garbage collector in some situations to cleanup tensors that might be
dangling. In torch’s implementation the R garbage collector is called
whenever reusing a cached block fails. In this case, GC is called and we
retry getting a new block. However, unlike in the CPU case, that
allocations failures are very rare, reusing a block is not common in
programs where LibTorch never reserves a large chunk of memory, causing
the GC to be called at almost every allocation (and calling GC is time
consuming).</p>
<p>To fix this, the torch allocator will call a faster GC in some
moments and make a full collection in others.</p>
<ol style="list-style-type: decimal">
<li>We don’t make any collection if the current reserved memory (cached
memory) divided by the total GPU memory is smaller than 20%. This can be
controlled by the <code>torch.cuda_allocator_reserved_rate</code> and
the default is 0.2.</li>
<li>We make a full collection if the current allocated memory (memory
used by tensors) divided by the total device memory is larger than 80%.
This can be controlled via the
<code>torch.cuda_allocator_allocated_rate</code> and the default is
0.8.</li>
<li>We make a full collection if the current allocated memory is larger
divided by current reserved memory is larger than 80%. This is
controlled by the
<code>torch.cuda_allocator_allocated_reserved_rate</code> and the
default is 0.8.</li>
<li>In all other cases a light collection is made. Equivalent to calling
<code>gc(full = FALSE</code>) in R.</li>
</ol>
<p>These options can help tuning allocation performance depending on the
program you are running.</p>
<div class="section level3">
<h3 id="cuda-memory-snapshots">CUDA Memory Snapshots<a class="anchor" aria-label="anchor" href="#cuda-memory-snapshots"></a>
</h3>
<p>To assist debugging CUDA memory usage, R torch provides functionality
for generating CUDA memory snapshots, similar to the <a href="https://docs.pytorch.org/docs/stable/torch_cuda_memory.html" class="external-link">PyTorch
Python implementation</a>. Snapshots record the state of allocated CUDA
memory at any point in time, and optionally records the history of
allocation events that led up to that snapshot.</p>
<p>To generate a snapshot:</p>
<pre><code><span><span class="co"># Enable memory history recording, capturing tracebacks and allocation events</span></span>
<span><span class="fu"><a href="../reference/cuda_record_memory_history.html">cuda_record_memory_history</a></span><span class="op">(</span>enabled <span class="op">=</span> <span class="st">"all"</span>, max_entries <span class="op">=</span> <span class="fl">1e6</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Run your code</span></span>
<span></span>
<span><span class="co"># Save a snapshot file</span></span>
<span><span class="fu"><a href="../reference/cuda_dump_memory_snapshot.html">cuda_dump_memory_snapshot</a></span><span class="op">(</span><span class="st">"my_snapshot.pickle"</span><span class="op">)</span></span></code></pre>
<p>Generated snapshots can be visualized interactively using the
official PyTorch Memory Visualizer at <a href="pytorch.org/memory_viz">pytorch.org/memory_viz</a>. Simply drag
and drop your saved snapshot (.pickle) into the visualizer, which runs
locally in your browser without uploading any data.</p>
</div>
<div class="section level3">
<h3 id="libtorch-cuda-allocation-options">LibTorch CUDA Allocation Options<a class="anchor" aria-label="anchor" href="#libtorch-cuda-allocation-options"></a>
</h3>
<p>Besides the R specific options you can set LibTorch options via
environment variables as described below. The behavior of caching
allocator can be controlled via environment variable
PYTORCH_CUDA_ALLOC_CONF. The format is
<code>PYTORCH_CUDA_ALLOC_CONF=&lt;option&gt;:&lt;value&gt;,&lt;option2&gt;&lt;value2&gt;..</code>.
Available options:</p>
<ul>
<li>
<code>max_split_size_mb</code> prevents the allocator from splitting
blocks larger than this size (in MB). This can help prevent
fragmentation and may allow some borderline workloads to complete
without running out of memory. Performance cost can range from ‘zero’ to
‘substatial’ depending on allocation patterns. Default value is
unlimited, i.e. all blocks can be split. The memory_stats() and
memory_summary() methods are useful for tuning. This option should be
used as a last resort for a workload that is aborting due to ‘out of
memory’ and showing a large amount of inactive split blocks.</li>
<li>
<code>roundup_power2_divisions</code> helps with rounding the
requested allocation size to nearest power-2 division and making better
use of the blocks. In the current CUDACachingAllocator, the sizes are
rounded up in multiple of blocks size of 512, so this works fine for
smaller sizes. However, this can be inefficient for large near-by
allocations as each will go to different size of blocks and re-use of
those blocks are minimized. This might create lots of unused blocks and
will waste GPU memory capacity. This option enables the rounding of
allocation size to nearest power-2 division. For example, if we need to
round-up size of 1200 and if number of divisions is 4, the size 1200
lies between 1024 and 2048 and if we do 4 divisions between them, the
values are 1024, 1280, 1536, and 1792. So, allocation size of 1200 will
be rounded to 1280 as the nearest ceiling of power-2 division.</li>
<li>
<code>garbage_collection_threshold</code> helps actively reclaiming
unused GPU memory to avoid triggering expensive sync-and-reclaim-all
operation (release_cached_blocks), which can be unfavorable to
latency-critical GPU applications (e.g., servers). Upon setting this
threshold (e.g., 0.8), the allocator will start reclaiming GPU memory
blocks if the GPU memory capacity usage exceeds the threshold (i.e., 80%
of the total memory allocated to the GPU application). The algorithm
prefers to free old &amp; unused blocks first to avoid freeing blocks
that are actively being reused. The threshold value should be between
greater than 0.0 and less than 1.0.</li>
</ul>
<p>Notice that the garbage collector refered below is not the R garbage
collector but LibTorch’s collector, that releases memory from the cache
to the OS.</p>
</div>
</div>
<div class="section level2">
<h2 id="mps">MPS<a class="anchor" aria-label="anchor" href="#mps"></a>
</h2>
<p>Memory management in MPS devices is very similar to the strategy used
in CUDA devices, except that here there’s currently no configuration or
tuning possible. The R garbage collector will be called whenever there’s
no more available memory for the GPU and thus, possibly deleting some
Tensors. Allocation is then retried and if it fails, a OOM error is
raised.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Daniel Falbel, Javier Luraschi.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
