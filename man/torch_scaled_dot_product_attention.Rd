% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gen-namespace-docs.R, R/gen-namespace.R
\name{torch_scaled_dot_product_attention}
\alias{torch_scaled_dot_product_attention}
\title{Scaled Dot Product Attention}
\usage{
torch_scaled_dot_product_attention(
  query,
  key,
  value,
  attn_mask = list(),
  dropout_p = 0L,
  is_causal = FALSE,
  scale = NULL,
  enable_gqa = FALSE
)
}
\arguments{
\item{query}{(Tensor) Query tensor; shape \eqn{(N, ..., L, E)}.}

\item{key}{(Tensor) Key tensor; shape \eqn{(N, ..., S, E)}.}

\item{value}{(Tensor) Value tensor; shape \eqn{(N, ..., S, Ev)}.}

\item{attn_mask}{(Tensor, optional) Attention mask; shape must be broadcastable to
the shape of attention weights, which is \eqn{(N,..., L, S)}. Two types of masks
are supported. A boolean mask where a value of \code{TRUE} indicates that the element
should take part in attention (and \code{FALSE} masks out the position). A float mask
of the same type as query, key, value that is added to the attention score (use
\code{-Inf} to mask out positions). Default: \code{list()}.}

\item{dropout_p}{(float) Dropout probability in the range [0.0, 1.0]; if greater
than 0.0, dropout is applied during training. Default: 0.0.}

\item{is_causal}{(bool) If \code{TRUE}, assumes causal attention masking. \code{attn_mask} is
ignored when \code{is_causal=TRUE}. Default: \code{FALSE}.}

\item{scale}{(float, optional) Scaling factor applied prior to softmax. If \code{NULL},
the default value is set to \eqn{1/\sqrt{E}}. Default: \code{NULL}.}

\item{enable_gqa}{(bool) If \code{TRUE}, enables grouped query attention (GQA) support.
Default: \code{FALSE}.}
}
\value{
A tensor with shape \eqn{(N, ..., L, Ev)}.
}
\description{
Computes scaled dot product attention on query, key and value tensors, using
an optional attention mask if passed, and applying dropout if a probability
greater than 0.0 is specified.
}
\details{
This function uses optimized fused CUDA kernels when available, providing
significant performance improvements (2-3x faster) compared to manually
computing attention. It is particularly beneficial for transformer models.

The attention mechanism is defined as:
\deqn{Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V}

Where \eqn{N} is the batch size, \eqn{S} is the source sequence length,
\eqn{L} is the target sequence length, \eqn{E} is the embedding dimension of the query and key,
and \eqn{Ev} is the embedding dimension of the value.

The function automatically selects the best available implementation based on
hardware and input characteristics. On CUDA devices with compatible architectures,
it can use flash attention or memory-efficient attention kernels.
}
\examples{
if (torch_is_installed()) {
if (torch_is_installed()) {
  # Basic usage
  query <- torch_randn(2, 8, 10, 64)  # (batch, heads, seq_len, dim)
  key <- torch_randn(2, 8, 10, 64)
  value <- torch_randn(2, 8, 10, 64)

  output <- torch_scaled_dot_product_attention(query, key, value)

  # With causal masking (for autoregressive models)
  output <- torch_scaled_dot_product_attention(
    query, key, value,
    is_causal = TRUE
  )

  # With attention mask
  seq_len <- 10
  attn_mask <- torch_ones(seq_len, seq_len)
  attn_mask <- torch_tril(attn_mask)  # Lower triangular mask
  output <- torch_scaled_dot_product_attention(
    query, key, value,
    attn_mask = attn_mask
  )
}

}
}
